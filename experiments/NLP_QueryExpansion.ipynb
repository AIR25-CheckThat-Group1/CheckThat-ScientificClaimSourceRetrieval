{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from torch.nn.functional import normalize\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryExpander:\n",
    "    #def __init__(self, model_name='msmarco-distilbert-base-tas-b'):\n",
    "    def __init__(self, model_name='multi-qa-mpnet-base-cos-v1'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.word_pool = None\n",
    "        self.word_embeddings = None\n",
    "        self.tfidf_vectorizer = None\n",
    "        \n",
    "    def build_word_pool(self, papers_df, min_freq=5, max_terms=100):\n",
    "        \"\"\"Build a domain-specific word pool from the corpus\"\"\"\n",
    "        # Create text column if it doesn't exist\n",
    "        if 'text' not in papers_df.columns:\n",
    "            papers_df['text'] = papers_df['title'] + '. ' + papers_df['abstract']\n",
    "        \n",
    "        # Combine all paper texts\n",
    "        all_texts = papers_df['text'].tolist()\n",
    "        \n",
    "        # Initialize TF-IDF vectorizer\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(\n",
    "            max_features=max_terms,\n",
    "            min_df=min_freq,\n",
    "            stop_words='english'\n",
    "        )\n",
    "        \n",
    "        # Fit and transform the texts\n",
    "        tfidf_matrix = self.tfidf_vectorizer.fit_transform(all_texts)\n",
    "        \n",
    "        # Get feature names (terms)\n",
    "        self.word_pool = self.tfidf_vectorizer.get_feature_names_out()\n",
    "        \n",
    "        # Pre-compute embeddings for the word pool\n",
    "        self.word_embeddings = self.model.encode(\n",
    "            self.word_pool.tolist(),\n",
    "            convert_to_tensor=True,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        \n",
    "        return self.word_pool\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and normalize text\"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # remove URLs\n",
    "        text = re.sub(r\"[@#]\\w+\", \"\", text)         # remove @mentions and #hashtags\n",
    "        text = re.sub(r\"[^\\w\\s\\-/]\", \"\", text)      # keep alphanum + dash/slash\n",
    "        return text.strip()\n",
    "    \n",
    "    def expand_query(self, query, top_n=3, expansion_weight=0.3):\n",
    "        \"\"\"\n",
    "        Expand query using semantic similarity and TF-IDF weighting\n",
    "        \n",
    "        Args:\n",
    "            query: Original query text\n",
    "            top_n: Number of terms to add\n",
    "            expansion_weight: Weight for expanded terms (0-1)\n",
    "        \"\"\"\n",
    "        # Clean the query\n",
    "        clean_query = self.clean_text(query)\n",
    "        \n",
    "        # Get query embedding\n",
    "        query_emb = self.model.encode(clean_query, convert_to_tensor=True)\n",
    "        \n",
    "        # Calculate similarity scores\n",
    "        scores = util.cos_sim(query_emb, self.word_embeddings)[0]\n",
    "        \n",
    "        # Get top terms\n",
    "        top_ids = scores.topk(top_n).indices\n",
    "        top_terms = [self.word_pool[i] for i in top_ids]\n",
    "        \n",
    "        # Calculate term weights based on similarity scores\n",
    "        term_weights = scores[top_ids].tolist()\n",
    "        term_weights = [w * expansion_weight for w in term_weights]\n",
    "        \n",
    "        # Create weighted expansion\n",
    "        expanded_terms = []\n",
    "        for term, weight in zip(top_terms, term_weights):\n",
    "            # Repeat terms based on their weight\n",
    "            repeat_count = max(1, int(weight * 3))\n",
    "            expanded_terms.extend([term] * repeat_count)\n",
    "        \n",
    "        # Combine original query with expanded terms\n",
    "        expanded_query = f\"{clean_query} {' '.join(expanded_terms)}\"\n",
    "        \n",
    "        return expanded_query\n",
    "    \n",
    "    def batch_expand_queries(self, queries, top_n=3, expansion_weight=0.3):\n",
    "        \"\"\"Expand a batch of queries\"\"\"\n",
    "        return [self.expand_query(q, top_n, expansion_weight) for q in queries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalSystem:\n",
    "    def __init__(self, model_name='msmarco-distilbert-base-tas-b'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.paper_embeddings = None\n",
    "        self.papers_df = None\n",
    "        \n",
    "    def load_papers(self, papers_df):\n",
    "        \"\"\"Load and encode paper collection\"\"\"\n",
    "        self.papers_df = papers_df.copy()  # Create a copy to avoid modifying the original\n",
    "        if 'text' not in self.papers_df.columns:\n",
    "            self.papers_df['text'] = self.papers_df['title'] + '. ' + self.papers_df['abstract']\n",
    "        \n",
    "        # Encode papers\n",
    "        self.paper_embeddings = self.model.encode(\n",
    "            self.papers_df['text'].tolist(),\n",
    "            show_progress_bar=True,\n",
    "            convert_to_tensor=True\n",
    "        )\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        self.paper_embeddings = normalize(self.paper_embeddings, p=2, dim=1)\n",
    "        \n",
    "    def get_topk_predictions(self, query_embeddings, top_k=5, batch_size=16):\n",
    "        \"\"\"Get top-k predictions using batched processing\"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for start_idx in range(0, len(query_embeddings), batch_size):\n",
    "            end_idx = min(start_idx + batch_size, len(query_embeddings))\n",
    "            query_batch = query_embeddings[start_idx:end_idx]\n",
    "            query_norm = normalize(query_batch, p=2, dim=1)\n",
    "            \n",
    "            # Calculate similarity scores\n",
    "            similarity_matrix = torch.matmul(query_norm, self.paper_embeddings.T)\n",
    "            _, top_k_indices = torch.topk(similarity_matrix, k=top_k, dim=1)\n",
    "            \n",
    "            # Get predictions\n",
    "            batch_predictions = [\n",
    "                self.papers_df.iloc[indices.tolist()]['cord_uid'].tolist()\n",
    "                for indices in top_k_indices\n",
    "            ]\n",
    "            predictions.extend(batch_predictions)\n",
    "            \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performance(data, col_gold, col_pred, list_k=[1, 5, 10]):\n",
    "    \"\"\"Evaluate retrieval performance using MRR\"\"\"\n",
    "    d_performance = {}\n",
    "    for k in list_k:\n",
    "        scores = []\n",
    "        for _, row in data.iterrows():\n",
    "            gold = row[col_gold]\n",
    "            preds = row[col_pred]\n",
    "            if isinstance(preds, str):\n",
    "                try:\n",
    "                    preds = eval(preds)\n",
    "                except:\n",
    "                    preds = []\n",
    "            if gold in preds[:k]:\n",
    "                rank = preds[:k].index(gold) + 1\n",
    "                scores.append(1.0 / rank)\n",
    "            else:\n",
    "                scores.append(0.0)\n",
    "        d_performance[k] = sum(scores) / len(scores) if scores else 0.0\n",
    "    return d_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load data\n",
    "    with open(\"/Users/mataonbas/AIR-CheckThat!-GroupProject/CheckThat-ScientificClaimSourceRetrieval/subtask4b_collection_data.pkl\", \"rb\") as f:\n",
    "        papers_df = pickle.load(f)\n",
    "    \n",
    "    train_df = pd.read_csv(\"/Users/mataonbas/AIR-CheckThat!-GroupProject/CheckThat-ScientificClaimSourceRetrieval/subtask4b_query_tweets_train.tsv\", sep=\"\\t\", \n",
    "                          names=[\"post_id\", \"tweet_text\", \"cord_uid\"])\n",
    "    dev_df = pd.read_csv(\"/Users/mataonbas/AIR-CheckThat!-GroupProject/CheckThat-ScientificClaimSourceRetrieval/subtask4b_query_tweets_dev.tsv\", sep=\"\\t\", \n",
    "                        names=[\"post_id\", \"tweet_text\", \"cord_uid\"])\n",
    "    \n",
    "    # Initialize systems\n",
    "    expander = QueryExpander()\n",
    "    retrieval = RetrievalSystem()\n",
    "    \n",
    "    # Build word pool and load papers\n",
    "    expander.build_word_pool(papers_df)\n",
    "    retrieval.load_papers(papers_df)\n",
    "    \n",
    "    # Expand queries\n",
    "    train_df['expanded_text'] = expander.batch_expand_queries(\n",
    "        train_df['tweet_text'].tolist(),\n",
    "        top_n=3,\n",
    "        expansion_weight=0.3\n",
    "    )\n",
    "    dev_df['expanded_text'] = expander.batch_expand_queries(\n",
    "        dev_df['tweet_text'].tolist(),\n",
    "        top_n=3,\n",
    "        expansion_weight=0.3\n",
    "    )\n",
    "    \n",
    "    # Encode expanded queries\n",
    "    train_query_embeddings = retrieval.model.encode(\n",
    "        train_df['expanded_text'].tolist(),\n",
    "        show_progress_bar=True,\n",
    "        convert_to_tensor=True\n",
    "    )\n",
    "    dev_query_embeddings = retrieval.model.encode(\n",
    "        dev_df['expanded_text'].tolist(),\n",
    "        show_progress_bar=True,\n",
    "        convert_to_tensor=True\n",
    "    )\n",
    "    \n",
    "    # Get predictions\n",
    "    train_df['preds'] = retrieval.get_topk_predictions(train_query_embeddings)\n",
    "    dev_df['preds'] = retrieval.get_topk_predictions(dev_query_embeddings)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_mrr = evaluate_performance(train_df, 'cord_uid', 'preds')\n",
    "    dev_mrr = evaluate_performance(dev_df, 'cord_uid', 'preds')\n",
    "    \n",
    "    print(\"Train MRR:\", train_mrr)\n",
    "    print(\"Dev MRR:\", dev_mrr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 4/4 [00:00<00:00, 12.64it/s]\n",
      "Batches: 100%|██████████| 242/242 [11:25<00:00,  2.83s/it]\n",
      "Batches: 100%|██████████| 402/402 [01:58<00:00,  3.38it/s]\n",
      "Batches: 100%|██████████| 44/44 [00:12<00:00,  3.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MRR: {1: 0.33016959701260307, 5: 0.39050490119806874, 10: 0.39050490119806874}\n",
      "Dev MRR: {1: 0.33547466095645967, 5: 0.38929336188436786, 10: 0.38929336188436786}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbert-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
