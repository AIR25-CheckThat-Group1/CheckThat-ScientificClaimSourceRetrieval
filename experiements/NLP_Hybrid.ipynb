{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHybrid Retrieval System for Scientific Claim Source Retrieval\\n\\nThis implementation combines Query Expansion and Multi-Hop approaches to improve\\nretrieval performance for matching tweets with relevant scientific papers.\\n\\nKey Features:\\n1. Query Expansion: Enhances queries with semantically similar terms from a domain-specific word pool\\n2. Multi-Hop Enrichment: Uses a two-step process to enrich query representations\\n3. Hybrid Approach: Combines both lexical and semantic information for better retrieval\\n\\nThe system uses SBERT for encoding and cosine similarity for matching.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Hybrid Retrieval System for Scientific Claim Source Retrieval\n",
    "\n",
    "This implementation combines Query Expansion and Multi-Hop approaches to improve\n",
    "retrieval performance for matching tweets with relevant scientific papers.\n",
    "\n",
    "Key Features:\n",
    "1. Query Expansion: Enhances queries with semantically similar terms from a domain-specific word pool\n",
    "2. Multi-Hop Enrichment: Uses a two-step process to enrich query representations\n",
    "3. Hybrid Approach: Combines both lexical and semantic information for better retrieval\n",
    "\n",
    "The system uses SBERT for encoding and cosine similarity for matching.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/sbert-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from torch.nn.functional import normalize\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridRetrievalSystem:\n",
    "    \"\"\"\n",
    "    A hybrid retrieval system that combines Query Expansion and Multi-Hop approaches.\n",
    "    \n",
    "    This system is designed to improve the matching between informal tweets and\n",
    "    formal scientific papers by:\n",
    "    1. Expanding queries with relevant domain terms\n",
    "    2. Enriching query representations through multi-hop reasoning\n",
    "    3. Combining both approaches for better retrieval performance\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='multi-qa-mpnet-base-cos-v1'):\n",
    "        \"\"\"\n",
    "        Initialize the hybrid retrieval system.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Name of the SBERT model to use for encoding\n",
    "        \"\"\"\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.paper_embeddings = None  # Pre-computed embeddings for all papers\n",
    "        self.papers_df = None        # DataFrame containing paper information\n",
    "        self.word_pool = None        # Domain-specific vocabulary\n",
    "        self.word_embeddings = None  # Pre-computed embeddings for word pool\n",
    "        self.tfidf_vectorizer = None # TF-IDF vectorizer for word pool creation\n",
    "        \n",
    "    def build_word_pool(self, papers_df, min_freq=5, max_terms=100):\n",
    "        \"\"\"\n",
    "        Build a domain-specific word pool from the corpus using TF-IDF.\n",
    "        \n",
    "        This method:\n",
    "        1. Extracts text from paper titles and abstracts\n",
    "        2. Uses TF-IDF to identify important domain terms\n",
    "        3. Pre-computes embeddings for these terms\n",
    "        \n",
    "        Args:\n",
    "            papers_df (pd.DataFrame): DataFrame containing paper information\n",
    "            min_freq (int): Minimum document frequency for terms\n",
    "            max_terms (int): Maximum number of terms to include\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Array of selected terms\n",
    "        \"\"\"\n",
    "        # Create text column if it doesn't exist\n",
    "        if 'text' not in papers_df.columns:\n",
    "            papers_df['text'] = papers_df['title'] + '. ' + papers_df['abstract']\n",
    "        \n",
    "        # Combine all paper texts\n",
    "        all_texts = papers_df['text'].tolist()\n",
    "        \n",
    "        # Initialize TF-IDF vectorizer\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(\n",
    "            max_features=max_terms,\n",
    "            min_df=min_freq,\n",
    "            stop_words='english'\n",
    "        )\n",
    "        \n",
    "        # Fit and transform the texts\n",
    "        tfidf_matrix = self.tfidf_vectorizer.fit_transform(all_texts)\n",
    "        \n",
    "        # Get feature names (terms)\n",
    "        self.word_pool = self.tfidf_vectorizer.get_feature_names_out()\n",
    "        \n",
    "        # Pre-compute embeddings for the word pool\n",
    "        self.word_embeddings = self.model.encode(\n",
    "            self.word_pool.tolist(),\n",
    "            convert_to_tensor=True,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        \n",
    "        return self.word_pool\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"\n",
    "        Clean and normalize text by removing URLs, mentions, and special characters.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text to clean\n",
    "            \n",
    "        Returns:\n",
    "            str: Cleaned text\n",
    "        \"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # remove URLs\n",
    "        text = re.sub(r\"[@#]\\w+\", \"\", text)         # remove @mentions and #hashtags\n",
    "        text = re.sub(r\"[^\\w\\s\\-/]\", \"\", text)      # keep alphanum + dash/slash\n",
    "        return text.strip()\n",
    "    \n",
    "    def expand_query(self, query, top_n=3, expansion_weight=0.3):\n",
    "        \"\"\"\n",
    "        Expand query using semantic similarity and TF-IDF weighting.\n",
    "        \n",
    "        This method:\n",
    "        1. Cleans the input query\n",
    "        2. Finds semantically similar terms from the word pool\n",
    "        3. Weights and adds these terms to create an expanded query\n",
    "        \n",
    "        Args:\n",
    "            query (str): Original query text\n",
    "            top_n (int): Number of terms to add\n",
    "            expansion_weight (float): Weight for expanded terms\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (expanded_query, query_embedding)\n",
    "        \"\"\"\n",
    "        # Clean the query\n",
    "        clean_query = self.clean_text(query)\n",
    "        \n",
    "        # Get query embedding\n",
    "        query_emb = self.model.encode(clean_query, convert_to_tensor=True)\n",
    "        \n",
    "        # Calculate similarity scores\n",
    "        scores = util.cos_sim(query_emb, self.word_embeddings)[0]\n",
    "        \n",
    "        # Get top terms\n",
    "        top_ids = scores.topk(top_n).indices\n",
    "        top_terms = [self.word_pool[i] for i in top_ids]\n",
    "        \n",
    "        # Calculate term weights based on similarity scores\n",
    "        term_weights = scores[top_ids].tolist()\n",
    "        term_weights = [w * expansion_weight for w in term_weights]\n",
    "        \n",
    "        # Create weighted expansion\n",
    "        expanded_terms = []\n",
    "        for term, weight in zip(top_terms, term_weights):\n",
    "            repeat_count = max(1, int(weight * 3))\n",
    "            expanded_terms.extend([term] * repeat_count)\n",
    "        \n",
    "        # Combine original query with expanded terms\n",
    "        expanded_query = f\"{clean_query} {' '.join(expanded_terms)}\"\n",
    "        \n",
    "        return expanded_query, query_emb\n",
    "    \n",
    "    def multi_hop_enrichment(self, query_emb, top_k=3, alpha=0.85):\n",
    "        \"\"\"\n",
    "        Enrich query embedding using multi-hop approach.\n",
    "        \n",
    "        This method:\n",
    "        1. Finds most similar documents to the query (first hop)\n",
    "        2. Uses these documents to enrich the query embedding (second hop)\n",
    "        3. Combines original and enriched embeddings\n",
    "        \n",
    "        Args:\n",
    "            query_emb (torch.Tensor): Query embedding\n",
    "            top_k (int): Number of documents to consider\n",
    "            alpha (float): Weight for original vs. enriched embedding\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Enriched query embedding\n",
    "        \"\"\"\n",
    "        # First hop: Find most similar documents\n",
    "        cos_scores = torch.nn.functional.cosine_similarity(\n",
    "            query_emb.unsqueeze(0), \n",
    "            self.paper_embeddings\n",
    "        ).squeeze()\n",
    "        \n",
    "        # Get top document embeddings\n",
    "        top_indices = torch.topk(cos_scores, k=top_k).indices\n",
    "        top_doc_embs = self.paper_embeddings[top_indices]\n",
    "        \n",
    "        # Second hop: Combine query with document information\n",
    "        avg_top_doc_emb = torch.mean(top_doc_embs, dim=0)\n",
    "        enriched_emb = alpha * query_emb + (1 - alpha) * avg_top_doc_emb\n",
    "        \n",
    "        return enriched_emb\n",
    "    \n",
    "    def hybrid_retrieval(self, query, top_k=5, batch_size=16, \n",
    "                        expansion_params={'top_n': 3, 'expansion_weight': 0.3},\n",
    "                        multi_hop_params={'top_k': 3, 'alpha': 0.85}):\n",
    "        \"\"\"\n",
    "        Combine query expansion and multi-hop approaches for retrieval.\n",
    "        \n",
    "        This method:\n",
    "        1. Expands the query with relevant terms\n",
    "        2. Enriches the query embedding through multi-hop\n",
    "        3. Uses the enriched embedding for final retrieval\n",
    "        \n",
    "        Args:\n",
    "            query (str): Original query text\n",
    "            top_k (int): Number of results to return\n",
    "            batch_size (int): Batch size for processing\n",
    "            expansion_params (dict): Parameters for query expansion\n",
    "            multi_hop_params (dict): Parameters for multi-hop enrichment\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (predictions, expanded_query)\n",
    "        \"\"\"\n",
    "        # Step 1: Query Expansion\n",
    "        expanded_query, query_emb = self.expand_query(\n",
    "            query, \n",
    "            top_n=expansion_params['top_n'],\n",
    "            expansion_weight=expansion_params['expansion_weight']\n",
    "        )\n",
    "        \n",
    "        # Step 2: Multi-Hop Enrichment\n",
    "        enriched_emb = self.multi_hop_enrichment(\n",
    "            query_emb,\n",
    "            top_k=multi_hop_params['top_k'],\n",
    "            alpha=multi_hop_params['alpha']\n",
    "        )\n",
    "        \n",
    "        # Step 3: Get predictions using enriched embedding\n",
    "        query_norm = normalize(enriched_emb.unsqueeze(0), p=2, dim=1)\n",
    "        similarity_matrix = torch.matmul(query_norm, self.paper_embeddings.T)\n",
    "        _, top_k_indices = torch.topk(similarity_matrix, k=top_k, dim=1)\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = self.papers_df.iloc[top_k_indices[0].tolist()]['cord_uid'].tolist()\n",
    "        \n",
    "        return predictions, expanded_query\n",
    "    \n",
    "    def load_papers(self, papers_df):\n",
    "        \"\"\"\n",
    "        Load and encode paper collection.\n",
    "        \n",
    "        This method:\n",
    "        1. Stores paper information\n",
    "        2. Computes embeddings for all papers\n",
    "        3. Normalizes embeddings for better similarity matching\n",
    "        \n",
    "        Args:\n",
    "            papers_df (pd.DataFrame): DataFrame containing paper information\n",
    "        \"\"\"\n",
    "        self.papers_df = papers_df.copy()\n",
    "        if 'text' not in self.papers_df.columns:\n",
    "            self.papers_df['text'] = self.papers_df['title'] + '. ' + self.papers_df['abstract']\n",
    "        \n",
    "        # Encode papers\n",
    "        self.paper_embeddings = self.model.encode(\n",
    "            self.papers_df['text'].tolist(),\n",
    "            show_progress_bar=True,\n",
    "            convert_to_tensor=True\n",
    "        )\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        self.paper_embeddings = normalize(self.paper_embeddings, p=2, dim=1)\n",
    "    \n",
    "    def batch_hybrid_retrieval(self, queries, top_k=5, batch_size=16,\n",
    "                             expansion_params={'top_n': 3, 'expansion_weight': 0.3},\n",
    "                             multi_hop_params={'top_k': 3, 'alpha': 0.85}):\n",
    "        \"\"\"\n",
    "        Process a batch of queries using hybrid retrieval.\n",
    "        \n",
    "        Args:\n",
    "            queries (list): List of query texts\n",
    "            top_k (int): Number of results to return per query\n",
    "            batch_size (int): Batch size for processing\n",
    "            expansion_params (dict): Parameters for query expansion\n",
    "            multi_hop_params (dict): Parameters for multi-hop enrichment\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (list of predictions, list of expanded queries)\n",
    "        \"\"\"\n",
    "        all_predictions = []\n",
    "        expanded_queries = []\n",
    "        \n",
    "        for query in queries:\n",
    "            predictions, expanded_query = self.hybrid_retrieval(\n",
    "                query,\n",
    "                top_k=top_k,\n",
    "                batch_size=batch_size,\n",
    "                expansion_params=expansion_params,\n",
    "                multi_hop_params=multi_hop_params\n",
    "            )\n",
    "            all_predictions.append(predictions)\n",
    "            expanded_queries.append(expanded_query)\n",
    "            \n",
    "        return all_predictions, expanded_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performance(data, col_gold, col_pred, list_k=[1, 5, 10]):\n",
    "    \"\"\"\n",
    "    Evaluate retrieval performance using Mean Reciprocal Rank (MRR).\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame containing predictions and gold labels\n",
    "        col_gold (str): Column name for gold labels\n",
    "        col_pred (str): Column name for predictions\n",
    "        list_k (list): List of k values for evaluation\n",
    "        \n",
    "    Returns:\n",
    "        dict: MRR scores for each k value\n",
    "    \"\"\"\n",
    "    d_performance = {}\n",
    "    for k in list_k:\n",
    "        scores = []\n",
    "        for _, row in data.iterrows():\n",
    "            gold = row[col_gold]\n",
    "            preds = row[col_pred]\n",
    "            if isinstance(preds, str):\n",
    "                try:\n",
    "                    preds = eval(preds)\n",
    "                except:\n",
    "                    preds = []\n",
    "            if gold in preds[:k]:\n",
    "                rank = preds[:k].index(gold) + 1\n",
    "                scores.append(1.0 / rank)\n",
    "            else:\n",
    "                scores.append(0.0)\n",
    "        d_performance[k] = sum(scores) / len(scores) if scores else 0.0\n",
    "    return d_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the hybrid retrieval system.\n",
    "    \n",
    "    This function:\n",
    "    1. Loads the data\n",
    "    2. Initializes the system\n",
    "    3. Processes queries\n",
    "    4. Evaluates performance\n",
    "    5. Prints results and examples\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    with open(\"/Users/mataonbas/AIR-CheckThat!-GroupProject/CheckThat-ScientificClaimSourceRetrieval/subtask4b_collection_data.pkl\", \"rb\") as f:\n",
    "        papers_df = pickle.load(f)\n",
    "    \n",
    "    train_df = pd.read_csv(\"/Users/mataonbas/AIR-CheckThat!-GroupProject/CheckThat-ScientificClaimSourceRetrieval/subtask4b_query_tweets_train.tsv\", sep=\"\\t\", \n",
    "                          names=[\"post_id\", \"tweet_text\", \"cord_uid\"])\n",
    "    dev_df = pd.read_csv(\"/Users/mataonbas/AIR-CheckThat!-GroupProject/CheckThat-ScientificClaimSourceRetrieval/subtask4b_query_tweets_dev.tsv\", sep=\"\\t\", \n",
    "                        names=[\"post_id\", \"tweet_text\", \"cord_uid\"])\n",
    "    \n",
    "    # Initialize hybrid system\n",
    "    hybrid_system = HybridRetrievalSystem()\n",
    "    \n",
    "    # Build word pool and load papers\n",
    "    hybrid_system.build_word_pool(papers_df)\n",
    "    hybrid_system.load_papers(papers_df)\n",
    "    \n",
    "    # Process queries\n",
    "    train_predictions, train_expanded = hybrid_system.batch_hybrid_retrieval(\n",
    "        train_df['tweet_text'].tolist(),\n",
    "        expansion_params={'top_n': 3, 'expansion_weight': 0.3},\n",
    "        multi_hop_params={'top_k': 3, 'alpha': 0.85}\n",
    "    )\n",
    "    \n",
    "    dev_predictions, dev_expanded = hybrid_system.batch_hybrid_retrieval(\n",
    "        dev_df['tweet_text'].tolist(),\n",
    "        expansion_params={'top_n': 3, 'expansion_weight': 0.3},\n",
    "        multi_hop_params={'top_k': 3, 'alpha': 0.85}\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    train_df['preds'] = train_predictions\n",
    "    train_df['expanded_text'] = train_expanded\n",
    "    dev_df['preds'] = dev_predictions\n",
    "    dev_df['expanded_text'] = dev_expanded\n",
    "    \n",
    "    # Evaluate\n",
    "    train_mrr = evaluate_performance(train_df, 'cord_uid', 'preds')\n",
    "    dev_mrr = evaluate_performance(dev_df, 'cord_uid', 'preds')\n",
    "    \n",
    "    print(\"Train MRR:\", train_mrr)\n",
    "    print(\"Dev MRR:\", dev_mrr)\n",
    "    \n",
    "    # Print some examples\n",
    "    print(\"\\nExample Query Expansions:\")\n",
    "    for i in range(3):\n",
    "        print(f\"\\nOriginal: {train_df['tweet_text'].iloc[i]}\")\n",
    "        print(f\"Expanded: {train_df['expanded_text'].iloc[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 4/4 [00:00<00:00, 13.22it/s]\n",
      "Batches: 100%|██████████| 242/242 [23:58<00:00,  5.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MRR: {1: 0.4430527462268555, 5: 0.5072804833774134, 10: 0.5072804833774134}\n",
      "Dev MRR: {1: 0.4475374732334047, 5: 0.51289555079705, 10: 0.51289555079705}\n",
      "\n",
      "Example Query Expansions:\n",
      "\n",
      "Original: tweet_text\n",
      "Expanded: tweet_text transmission time use\n",
      "\n",
      "Original: Oral care in rehabilitation medicine: oral vulnerability, oral muscle wasting, and hospital-associated oral issues\n",
      "Expanded: oral care in rehabilitation medicine oral vulnerability oral muscle wasting and hospital-associated oral issues treatment patients hospital\n",
      "\n",
      "Original: this study isn't receiving sufficient attention. it reveals black/latino/indigenous individuals aren't just succumbing to covid at higher rates than whites but are also passing away *earlier*.   90% of white fatalities occur in those 65+, 90% of black fatalities occur in those 55+,  89% of indigenous fatalities occur in those 45+\n",
      "Expanded: this study isnt receiving sufficient attention it reveals black/latino/indigenous individuals arent just succumbing to covid at higher rates than whites but are also passing away earlier   90 of white fatalities occur in those 65 90 of black fatalities occur in those 55  89 of indigenous fatalities occur in those 45 mortality deaths covid\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbert-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
