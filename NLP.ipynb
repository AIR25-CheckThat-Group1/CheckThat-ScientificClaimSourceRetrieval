{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "000cc8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 12854 train tweets, 1401 dev tweets, 7718 papers.\n"
     ]
    }
   ],
   "source": [
    "# Load train & dev queries, and paper collection\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Load paper metadata\n",
    "with open(\"subtask4b_collection_data.pkl\", \"rb\") as f:\n",
    "    papers_df = pickle.load(f)\n",
    "\n",
    "papers_df[\"text\"] = papers_df[\"title\"] + \". \" + papers_df[\"abstract\"]\n",
    "\n",
    "# Load tweet queries\n",
    "train_df = pd.read_csv(\"subtask4b_query_tweets_train.tsv\", sep=\"\\t\", names=[\"post_id\", \"tweet_text\", \"cord_uid\"])\n",
    "dev_df = pd.read_csv(\"subtask4b_query_tweets_dev.tsv\", sep=\"\\t\", names=[\"post_id\", \"tweet_text\", \"cord_uid\"])\n",
    "\n",
    "print(f\"Loaded: {len(train_df)} train tweets, {len(dev_df)} dev tweets, {len(papers_df)} papers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "173d7f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SBERT model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "#model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "#model = SentenceTransformer('multi-qa-MiniLM-L6-dot-v1')\n",
    "#model = SentenceTransformer('allenai/specter')\n",
    "#model = SentenceTransformer('msmarco-distilbert-base-tas-b')\n",
    "\n",
    "#model = SentenceTransformer('multi-qa-mpnet-base-cos-v1')\n",
    "model = SentenceTransformer(\"/Users/mataonbas/AIR-CheckThat!-GroupProject/CheckThat-ScientificClaimSourceRetrieval/fine_tuned_sbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "afaccc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define cleaning function. This one is a bit agressive cleaning so result got worsen\n",
    "#def clean_text(text):\n",
    "#    text = text.lower()\n",
    "#    text = re.sub(r\"http\\\\S+|www\\\\S+\", \"\", text)\n",
    "#    text = re.sub(r\"[@#]\\\\w+\", \"\", text)\n",
    "#    text = re.sub(r\"[^a-z0-9\\\\s]\", \"\", text)\n",
    "#    return text.strip()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)         # remove URLs\n",
    "    text = re.sub(r\"[@#]\\w+\", \"\", text)                # remove @mentions and #hashtags\n",
    "    text = re.sub(r\"[^\\w\\s\\-\\/]\", \"\", text)            # keep alphanum + dash/slash\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9492913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning\n",
    "train_df[\"clean_tweet_text\"] = train_df[\"tweet_text\"].apply(clean_text)\n",
    "dev_df[\"clean_tweet_text\"] = dev_df[\"tweet_text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2df21db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 402/402 [03:45<00:00,  1.79it/s]\n",
      "Batches: 100%|██████████| 44/44 [00:23<00:00,  1.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# Encode cleaned queries\n",
    "train_query_embeddings = model.encode(\n",
    "    train_df[\"clean_tweet_text\"].tolist(),\n",
    "    show_progress_bar=True,\n",
    "    convert_to_tensor=True\n",
    ")\n",
    "\n",
    "dev_query_embeddings = model.encode(\n",
    "    dev_df[\"clean_tweet_text\"].tolist(),\n",
    "    show_progress_bar=True,\n",
    "    convert_to_tensor=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef7e99bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 242/242 [26:17<00:00,  6.52s/it] \n"
     ]
    }
   ],
   "source": [
    "# Encode document collection\n",
    "paper_embeddings = model.encode(papers_df[\"text\"].tolist(), show_progress_bar=True, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "212d152b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save paper embeddings to avoid recomputing in future runs\n",
    "import torch\n",
    "torch.save(paper_embeddings, \"paper_embeddings.pt\")\n",
    "\n",
    "# To load later:\n",
    "#paper_embeddings = torch.load(\"paper_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1187206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cosine_similarity\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "import torch\n",
    "\n",
    "#default\n",
    "#def get_topk_predictions_from_embeddings(query_embeddings, top_k=5):\n",
    "#    results = []\n",
    "#    for query_emb in query_embeddings:\n",
    "#        cos_scores = cosine_similarity(query_emb.unsqueeze(0), paper_embeddings).squeeze()\n",
    "#        top_indices = torch.topk(cos_scores, k=min(top_k, len(paper_embeddings))).indices.tolist()\n",
    "#        top_cord_uids = papers_df.iloc[top_indices][\"cord_uid\"].tolist()\n",
    "#        results.append(top_cord_uids)\n",
    "#    return results\n",
    "\n",
    "\n",
    "def get_topk_predictions_batched(query_embeddings, paper_embeddings, papers_df, top_k=5, batch_size=16):\n",
    "    predictions = []\n",
    "    paper_norm = normalize(paper_embeddings, p=2, dim=1)\n",
    "\n",
    "    for start_idx in range(0, len(query_embeddings), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(query_embeddings))\n",
    "        query_batch = query_embeddings[start_idx:end_idx]\n",
    "        query_norm = normalize(query_batch, p=2, dim=1)\n",
    "\n",
    "        similarity_matrix = torch.matmul(query_norm, paper_norm.T)\n",
    "        _, top_k_indices = torch.topk(similarity_matrix, k=top_k, dim=1)\n",
    "\n",
    "        batch_predictions = [\n",
    "            papers_df.iloc[indices.tolist()][\"cord_uid\"].tolist()\n",
    "            for indices in top_k_indices\n",
    "        ]\n",
    "        predictions.extend(batch_predictions)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf5283e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run predictions default\n",
    "#train_df[\"preds\"] = get_topk_predictions_from_embeddings(train_query_embeddings)\n",
    "#dev_df[\"preds\"] = get_topk_predictions_from_embeddings(dev_query_embeddings)\n",
    "\n",
    "train_df[\"preds\"] = get_topk_predictions_batched(train_query_embeddings, paper_embeddings, papers_df)\n",
    "dev_df[\"preds\"] = get_topk_predictions_batched(dev_query_embeddings, paper_embeddings, papers_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65895ac7",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cdfc21a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate MRR\n",
    "def get_performance_mrr(data, col_gold, col_pred, list_k=[1, 5, 10]):\n",
    "    d_performance = {}\n",
    "    for k in list_k:\n",
    "        scores = []\n",
    "        for _, row in data.iterrows():\n",
    "            gold = row[col_gold]\n",
    "            preds = row[col_pred]\n",
    "            if isinstance(preds, str):\n",
    "                try:\n",
    "                    preds = eval(preds)\n",
    "                except:\n",
    "                    preds = []\n",
    "            if gold in preds[:k]:\n",
    "                rank = preds[:k].index(gold) + 1\n",
    "                scores.append(1.0 / rank)\n",
    "            else:\n",
    "                scores.append(0.0)\n",
    "        d_performance[k] = sum(scores) / len(scores) if scores else 0.0\n",
    "    return d_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "34876725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MRR: {1: 0.6729422747782792, 5: 0.7570678388050404, 10: 0.7570678388050404}\n",
      "Dev MRR: {1: 0.5795860099928622, 5: 0.6492267428027606, 10: 0.6492267428027606}\n"
     ]
    }
   ],
   "source": [
    "# Print results default\n",
    "print(\"Train MRR:\", get_performance_mrr(train_df, \"cord_uid\", \"preds\"))\n",
    "print(\"Dev MRR:\", get_performance_mrr(dev_df, \"cord_uid\", \"preds\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbert-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
