{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58e2003b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import bm25s\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from mxbai_rerank import MxbaiRerankV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d91b9f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3070'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28ddfe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('data/')\n",
    "\n",
    "collection_data_path = data_path / 'subtask4b_collection_data.pkl' \n",
    "query_dev_data_path = data_path / 'subtask4b_query_tweets_dev.tsv'\n",
    "query_train_data_path = data_path / 'subtask4b_query_tweets_train.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cfb2779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>source_x</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>pubmed_id</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>mag_id</th>\n",
       "      <th>who_covidence_id</th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>label</th>\n",
       "      <th>time</th>\n",
       "      <th>timet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>umvrwgaw</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Professional and Home-Made Face Masks Reduce E...</td>\n",
       "      <td>10.1371/journal.pone.0002618</td>\n",
       "      <td>PMC2440799</td>\n",
       "      <td>18612429</td>\n",
       "      <td>cc-by</td>\n",
       "      <td>BACKGROUND: Governments are preparing for a po...</td>\n",
       "      <td>2008-07-09</td>\n",
       "      <td>van der Sande, Marianne; Teunis, Peter; Sabel,...</td>\n",
       "      <td>PLoS One</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>umvrwgaw</td>\n",
       "      <td>2008-07-09</td>\n",
       "      <td>1215561600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>spiud6ok</td>\n",
       "      <td>PMC</td>\n",
       "      <td>The Failure of R (0)</td>\n",
       "      <td>10.1155/2011/527610</td>\n",
       "      <td>PMC3157160</td>\n",
       "      <td>21860658</td>\n",
       "      <td>cc-by</td>\n",
       "      <td>The basic reproductive ratio, R (0), is one of...</td>\n",
       "      <td>2011-08-16</td>\n",
       "      <td>Li, Jing; Blakeley, Daniel; Smith?, Robert J.</td>\n",
       "      <td>Comput Math Methods Med</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>spiud6ok</td>\n",
       "      <td>2011-08-16</td>\n",
       "      <td>1313452800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>aclzp3iy</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Pulmonary sequelae in a patient recovered from...</td>\n",
       "      <td>10.4103/0970-2113.99118</td>\n",
       "      <td>PMC3424870</td>\n",
       "      <td>22919170</td>\n",
       "      <td>cc-by-nc-sa</td>\n",
       "      <td>The pandemic of swine flu (H1N1) influenza spr...</td>\n",
       "      <td>2012</td>\n",
       "      <td>Singh, Virendra; Sharma, Bharat Bhushan; Patel...</td>\n",
       "      <td>Lung India</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aclzp3iy</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>1325376000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>ycxyn2a2</td>\n",
       "      <td>PMC</td>\n",
       "      <td>What was the primary mode of smallpox transmis...</td>\n",
       "      <td>10.3389/fcimb.2012.00150</td>\n",
       "      <td>PMC3509329</td>\n",
       "      <td>23226686</td>\n",
       "      <td>cc-by</td>\n",
       "      <td>The mode of infection transmission has profoun...</td>\n",
       "      <td>2012-11-29</td>\n",
       "      <td>Milton, Donald K.</td>\n",
       "      <td>Front Cell Infect Microbiol</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ycxyn2a2</td>\n",
       "      <td>2012-11-29</td>\n",
       "      <td>1354147200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>zxe95qy9</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Lessons from the History of Quarantine, from P...</td>\n",
       "      <td>10.3201/eid1902.120312</td>\n",
       "      <td>PMC3559034</td>\n",
       "      <td>23343512</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>In the new millennium, the centuries-old strat...</td>\n",
       "      <td>2013-02-03</td>\n",
       "      <td>Tognotti, Eugenia</td>\n",
       "      <td>Emerg Infect Dis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>zxe95qy9</td>\n",
       "      <td>2013-02-03</td>\n",
       "      <td>1359849600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cord_uid source_x                                              title  \\\n",
       "162   umvrwgaw      PMC  Professional and Home-Made Face Masks Reduce E...   \n",
       "611   spiud6ok      PMC                               The Failure of R (0)   \n",
       "918   aclzp3iy      PMC  Pulmonary sequelae in a patient recovered from...   \n",
       "993   ycxyn2a2      PMC  What was the primary mode of smallpox transmis...   \n",
       "1053  zxe95qy9      PMC  Lessons from the History of Quarantine, from P...   \n",
       "\n",
       "                               doi       pmcid pubmed_id      license  \\\n",
       "162   10.1371/journal.pone.0002618  PMC2440799  18612429        cc-by   \n",
       "611            10.1155/2011/527610  PMC3157160  21860658        cc-by   \n",
       "918        10.4103/0970-2113.99118  PMC3424870  22919170  cc-by-nc-sa   \n",
       "993       10.3389/fcimb.2012.00150  PMC3509329  23226686        cc-by   \n",
       "1053        10.3201/eid1902.120312  PMC3559034  23343512        no-cc   \n",
       "\n",
       "                                               abstract publish_time  \\\n",
       "162   BACKGROUND: Governments are preparing for a po...   2008-07-09   \n",
       "611   The basic reproductive ratio, R (0), is one of...   2011-08-16   \n",
       "918   The pandemic of swine flu (H1N1) influenza spr...         2012   \n",
       "993   The mode of infection transmission has profoun...   2012-11-29   \n",
       "1053  In the new millennium, the centuries-old strat...   2013-02-03   \n",
       "\n",
       "                                                authors  \\\n",
       "162   van der Sande, Marianne; Teunis, Peter; Sabel,...   \n",
       "611       Li, Jing; Blakeley, Daniel; Smith?, Robert J.   \n",
       "918   Singh, Virendra; Sharma, Bharat Bhushan; Patel...   \n",
       "993                                   Milton, Donald K.   \n",
       "1053                                  Tognotti, Eugenia   \n",
       "\n",
       "                          journal  mag_id who_covidence_id arxiv_id     label  \\\n",
       "162                      PLoS One     NaN              NaN      NaN  umvrwgaw   \n",
       "611       Comput Math Methods Med     NaN              NaN      NaN  spiud6ok   \n",
       "918                    Lung India     NaN              NaN      NaN  aclzp3iy   \n",
       "993   Front Cell Infect Microbiol     NaN              NaN      NaN  ycxyn2a2   \n",
       "1053             Emerg Infect Dis     NaN              NaN      NaN  zxe95qy9   \n",
       "\n",
       "           time       timet  \n",
       "162  2008-07-09  1215561600  \n",
       "611  2011-08-16  1313452800  \n",
       "918  2012-01-01  1325376000  \n",
       "993  2012-11-29  1354147200  \n",
       "1053 2013-02-03  1359849600  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_collection = pd.read_pickle(collection_data_path)\n",
    "df_collection.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02fe1d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cord_uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>covid recovery: this study from the usa reveal...</td>\n",
       "      <td>3qvh482o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69</td>\n",
       "      <td>\"Among 139 clients exposed to two symptomatic ...</td>\n",
       "      <td>r58aohnu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73</td>\n",
       "      <td>I recall early on reading that researchers who...</td>\n",
       "      <td>sts48u9i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93</td>\n",
       "      <td>You know you're credible when NIH website has ...</td>\n",
       "      <td>3sr2exq9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>96</td>\n",
       "      <td>Resistance to antifungal medications is a grow...</td>\n",
       "      <td>ybwwmyqy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>14193</td>\n",
       "      <td>Residents at high risk of covid-19: effectiven...</td>\n",
       "      <td>0gn3b98n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>14196</td>\n",
       "      <td>61% of teenagers hospitalized for covid were \"...</td>\n",
       "      <td>25bdifv6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>14203</td>\n",
       "      <td>\"fresh evidence backing melatonin against covi...</td>\n",
       "      <td>qn6wawxk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>14233</td>\n",
       "      <td>the vaccine doesn't halt the spread, it is pro...</td>\n",
       "      <td>3u3i5myh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>14236</td>\n",
       "      <td>\"Great commentary from K. Carvalho,  black pre...</td>\n",
       "      <td>nih4l4ok</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      post_id                                         tweet_text  cord_uid\n",
       "0          16  covid recovery: this study from the usa reveal...  3qvh482o\n",
       "1          69  \"Among 139 clients exposed to two symptomatic ...  r58aohnu\n",
       "2          73  I recall early on reading that researchers who...  sts48u9i\n",
       "3          93  You know you're credible when NIH website has ...  3sr2exq9\n",
       "4          96  Resistance to antifungal medications is a grow...  ybwwmyqy\n",
       "...       ...                                                ...       ...\n",
       "1395    14193  Residents at high risk of covid-19: effectiven...  0gn3b98n\n",
       "1396    14196  61% of teenagers hospitalized for covid were \"...  25bdifv6\n",
       "1397    14203  \"fresh evidence backing melatonin against covi...  qn6wawxk\n",
       "1398    14233  the vaccine doesn't halt the spread, it is pro...  3u3i5myh\n",
       "1399    14236  \"Great commentary from K. Carvalho,  black pre...  nih4l4ok\n",
       "\n",
       "[1400 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cord_uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Oral care in rehabilitation medicine: oral vul...</td>\n",
       "      <td>htlvpvz5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>this study isn't receiving sufficient attentio...</td>\n",
       "      <td>4kfl29ul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>thanks, xi jinping. a reminder that this study...</td>\n",
       "      <td>jtwb17u8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Taiwan - a population of 23 million has had ju...</td>\n",
       "      <td>0w9k8iy1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Obtaining a diagnosis of autism in lower incom...</td>\n",
       "      <td>tiqksd69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12848</th>\n",
       "      <td>14248</td>\n",
       "      <td>\"evidence on covid-19 reveals a growing body o...</td>\n",
       "      <td>9169o29b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12849</th>\n",
       "      <td>14249</td>\n",
       "      <td>Outdoor lighting has detrimental impacts on lo...</td>\n",
       "      <td>s2bpha8l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12850</th>\n",
       "      <td>14250</td>\n",
       "      <td>26/ and influenza virus (and other pathogens, ...</td>\n",
       "      <td>atloc9th</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12851</th>\n",
       "      <td>14251</td>\n",
       "      <td>does it?'sars-cov-2-naïve vaccinees had a 13.0...</td>\n",
       "      <td>t4y1ylb3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12852</th>\n",
       "      <td>14252</td>\n",
       "      <td>when \"the airway immune cells of children are ...</td>\n",
       "      <td>nlsv8bin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12853 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       post_id                                         tweet_text  cord_uid\n",
       "0            0  Oral care in rehabilitation medicine: oral vul...  htlvpvz5\n",
       "1            1  this study isn't receiving sufficient attentio...  4kfl29ul\n",
       "2            2  thanks, xi jinping. a reminder that this study...  jtwb17u8\n",
       "3            3  Taiwan - a population of 23 million has had ju...  0w9k8iy1\n",
       "4            4  Obtaining a diagnosis of autism in lower incom...  tiqksd69\n",
       "...        ...                                                ...       ...\n",
       "12848    14248  \"evidence on covid-19 reveals a growing body o...  9169o29b\n",
       "12849    14249  Outdoor lighting has detrimental impacts on lo...  s2bpha8l\n",
       "12850    14250  26/ and influenza virus (and other pathogens, ...  atloc9th\n",
       "12851    14251  does it?'sars-cov-2-naïve vaccinees had a 13.0...  t4y1ylb3\n",
       "12852    14252  when \"the airway immune cells of children are ...  nlsv8bin\n",
       "\n",
       "[12853 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_query_dev = pd.read_csv(query_dev_data_path, sep='\\t')\n",
    "df_query_train = pd.read_csv(query_train_data_path, sep='\\t')\n",
    "display(df_query_dev)\n",
    "display(df_query_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a720417a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cord_uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>covid recovery: this study from the usa reveal...</td>\n",
       "      <td>3qvh482o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69</td>\n",
       "      <td>\"Among 139 clients exposed to two symptomatic ...</td>\n",
       "      <td>r58aohnu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73</td>\n",
       "      <td>I recall early on reading that researchers who...</td>\n",
       "      <td>sts48u9i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93</td>\n",
       "      <td>You know you're credible when NIH website has ...</td>\n",
       "      <td>3sr2exq9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>96</td>\n",
       "      <td>Resistance to antifungal medications is a grow...</td>\n",
       "      <td>ybwwmyqy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14248</th>\n",
       "      <td>14248</td>\n",
       "      <td>\"evidence on covid-19 reveals a growing body o...</td>\n",
       "      <td>9169o29b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14249</th>\n",
       "      <td>14249</td>\n",
       "      <td>Outdoor lighting has detrimental impacts on lo...</td>\n",
       "      <td>s2bpha8l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14250</th>\n",
       "      <td>14250</td>\n",
       "      <td>26/ and influenza virus (and other pathogens, ...</td>\n",
       "      <td>atloc9th</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14251</th>\n",
       "      <td>14251</td>\n",
       "      <td>does it?'sars-cov-2-naïve vaccinees had a 13.0...</td>\n",
       "      <td>t4y1ylb3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14252</th>\n",
       "      <td>14252</td>\n",
       "      <td>when \"the airway immune cells of children are ...</td>\n",
       "      <td>nlsv8bin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14253 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       post_id                                         tweet_text  cord_uid\n",
       "0           16  covid recovery: this study from the usa reveal...  3qvh482o\n",
       "1           69  \"Among 139 clients exposed to two symptomatic ...  r58aohnu\n",
       "2           73  I recall early on reading that researchers who...  sts48u9i\n",
       "3           93  You know you're credible when NIH website has ...  3sr2exq9\n",
       "4           96  Resistance to antifungal medications is a grow...  ybwwmyqy\n",
       "...        ...                                                ...       ...\n",
       "14248    14248  \"evidence on covid-19 reveals a growing body o...  9169o29b\n",
       "14249    14249  Outdoor lighting has detrimental impacts on lo...  s2bpha8l\n",
       "14250    14250  26/ and influenza virus (and other pathogens, ...  atloc9th\n",
       "14251    14251  does it?'sars-cov-2-naïve vaccinees had a 13.0...  t4y1ylb3\n",
       "14252    14252  when \"the airway immune cells of children are ...  nlsv8bin\n",
       "\n",
       "[14253 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_query_combined = pd.concat([df_query_dev, df_query_train], ignore_index=True)\n",
    "df_query_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a66f44e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_collection['last_names'] = df_collection['authors'].str.split(';').apply(\n",
    "    lambda authors: [author.split(',')[0].strip() for author in authors] if isinstance(authors, list) else authors\n",
    ")\n",
    "df_collection['last_names'] = df_collection['last_names'].apply(\n",
    "    lambda x: '; '.join(x) if isinstance(x, list) else x\n",
    ")\n",
    "df_collection['publish_year'] = df_collection['publish_time'].str.split('-').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10c6aa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_bm25(corpus: list[str], cord_uids:list[str], k1=1.5, b=0.75, method='lucene', stemmer=None):\n",
    "    tokenized_corpus = bm25s.tokenize(corpus, stemmer=stemmer)\n",
    "    bm25 = bm25s.BM25(corpus=cord_uids, k1=k1, b=b, method=method)\n",
    "    bm25.index(tokenized_corpus)\n",
    "    return bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33950172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_single_bm25(df_collection, df_query, k1=1.5, b=0.75, stemmer=None, k=10):\n",
    "    corpus = df_collection.apply(\n",
    "        lambda x: f\"{x['title']} {x['abstract']} {x['last_names']} {x['journal']} {x['publish_year']}\", axis=1\n",
    "    ).tolist()\n",
    "    bm25 = initialize_bm25(corpus, df_collection['cord_uid'].tolist(), k1, b, stemmer=stemmer)\n",
    "    tokenized_queries = bm25s.tokenize(df_query['tweet_text'], stemmer=stemmer)\n",
    "    doc_scores = bm25.retrieve(tokenized_queries, n_threads=-1, k=k)\n",
    "    df_query['bm25_topk'] = doc_scores.documents.tolist()\n",
    "    \n",
    "    return df_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1d4677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_mrr(data, col_gold, col_pred, list_k = [1, 5, 10]):\n",
    "    d_performance = {}\n",
    "    for k in list_k:\n",
    "        data[\"in_topx\"] = data.apply(lambda x: (1/([i for i in x[col_pred][:k]].index(x[col_gold]) + 1) if x[col_gold] in [i for i in x[col_pred][:k]] else 0), axis=1)\n",
    "        d_performance[k] = data[\"in_topx\"].mean()\n",
    "        print(f\"{k = }\")\n",
    "        in_topx = data[\"in_topx\"] > 0\n",
    "        print(f\"Number of queries in top {k}: {in_topx.sum()}\")\n",
    "        print(f\"Number of queries not in top {k}: {len(data) - in_topx.sum()}\")\n",
    "    return d_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a580058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_experiment(df_query_train, df_query_dev, experiment_name, list_k=[1, 5, 10]):\n",
    "    results = get_performance_mrr(df_query_train, 'cord_uid', 'bm25_topk', list_k)\n",
    "    print(f\"Results for {experiment_name}, train: {results}\")\n",
    "    results = get_performance_mrr(df_query_dev, 'cord_uid', 'bm25_topk', list_k)\n",
    "    print(f\"Results for {experiment_name}, dev: {results}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dfb72d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1\n",
      "Number of queries in top 1: 7540\n",
      "Number of queries not in top 1: 5313\n",
      "k = 5\n",
      "Number of queries in top 5: 9096\n",
      "Number of queries not in top 5: 3757\n",
      "k = 10\n",
      "Number of queries in top 10: 9630\n",
      "Number of queries not in top 10: 3223\n",
      "k = 100\n",
      "Number of queries in top 100: 11157\n",
      "Number of queries not in top 100: 1696\n",
      "k = 200\n",
      "Number of queries in top 200: 11513\n",
      "Number of queries not in top 200: 1340\n",
      "k = 500\n",
      "Number of queries in top 500: 11957\n",
      "Number of queries not in top 500: 896\n",
      "k = 700\n",
      "Number of queries in top 700: 12090\n",
      "Number of queries not in top 700: 763\n",
      "k = 1000\n",
      "Number of queries in top 1000: 12211\n",
      "Number of queries not in top 1000: 642\n",
      "k = 2000\n",
      "Number of queries in top 2000: 12211\n",
      "Number of queries not in top 2000: 642\n",
      "k = 5000\n",
      "Number of queries in top 5000: 12211\n",
      "Number of queries not in top 5000: 642\n",
      "Results for Single BM25 with all features, train: {1: np.float64(0.5866334707850307), 5: np.float64(0.634745455016987), 10: np.float64(0.6402931685394925), 100: np.float64(0.6449074311828275), 200: np.float64(0.6451042573971111), 500: np.float64(0.6452173205148215), 700: np.float64(0.6452349451615127), 1000: np.float64(0.6452462313827287), 2000: np.float64(0.6452462313827287), 5000: np.float64(0.6452462313827287)}\n",
      "k = 1\n",
      "Number of queries in top 1: 829\n",
      "Number of queries not in top 1: 571\n",
      "k = 5\n",
      "Number of queries in top 5: 1004\n",
      "Number of queries not in top 5: 396\n",
      "k = 10\n",
      "Number of queries in top 10: 1060\n",
      "Number of queries not in top 10: 340\n",
      "k = 100\n",
      "Number of queries in top 100: 1222\n",
      "Number of queries not in top 100: 178\n",
      "k = 200\n",
      "Number of queries in top 200: 1266\n",
      "Number of queries not in top 200: 134\n",
      "k = 500\n",
      "Number of queries in top 500: 1300\n",
      "Number of queries not in top 500: 100\n",
      "k = 700\n",
      "Number of queries in top 700: 1323\n",
      "Number of queries not in top 700: 77\n",
      "k = 1000\n",
      "Number of queries in top 1000: 1336\n",
      "Number of queries not in top 1000: 64\n",
      "k = 2000\n",
      "Number of queries in top 2000: 1336\n",
      "Number of queries not in top 2000: 64\n",
      "k = 5000\n",
      "Number of queries in top 5000: 1336\n",
      "Number of queries not in top 5000: 64\n",
      "Results for Single BM25 with all features, dev: {1: np.float64(0.5921428571428572), 5: np.float64(0.6397619047619048), 10: np.float64(0.6450986394557824), 100: np.float64(0.6495240324689373), 200: np.float64(0.6497567832248164), 500: np.float64(0.6498352020281756), 700: np.float64(0.6498638803520272), 1000: np.float64(0.6498750349577517), 2000: np.float64(0.6498750349577517), 5000: np.float64(0.6498750349577517)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: np.float64(0.5921428571428572),\n",
       " 5: np.float64(0.6397619047619048),\n",
       " 10: np.float64(0.6450986394557824),\n",
       " 100: np.float64(0.6495240324689373),\n",
       " 200: np.float64(0.6497567832248164),\n",
       " 500: np.float64(0.6498352020281756),\n",
       " 700: np.float64(0.6498638803520272),\n",
       " 1000: np.float64(0.6498750349577517),\n",
       " 2000: np.float64(0.6498750349577517),\n",
       " 5000: np.float64(0.6498750349577517)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "stemmer = None\n",
    "df_query_train_single = experiment_single_bm25(df_collection, df_query_train, stemmer=stemmer, k=1000)\n",
    "df_query_dev_single = experiment_single_bm25(df_collection, df_query_dev, stemmer=stemmer, k=1000)\n",
    "evaluate_experiment(df_query_train_single, df_query_dev_single, \"Single BM25 with all features\", list_k=[1, 5, 10, 100, 200, 500, 700, 1000, 2000, 5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975a0b66",
   "metadata": {},
   "source": [
    "We will use BM25 as a baseline ranking model. The experiment above shouws us that there is no improvement after the first 1k (doesnt matter whether we take top 1k or top 2k etc.) -> so for each query we will prefilter the documents with BM25 to the top 1k and then the reranker will take over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eefbf4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_reranker(model_name, torch_dtype=torch.float16):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, trust_remote_code=True, torch_dtype=torch.float16\n",
    "    )\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Using GPU\")\n",
    "        model = model.to('cuda')\n",
    "    model.eval()\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25058bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_with_alibaba(df_query, df_collection, tokenizer, model, k=10, max_length=512, batch_size=32):\n",
    "    reranked_results = []\n",
    "    reranked_scores = []\n",
    "    reranked_docs = []\n",
    "\n",
    "    for _, row in tqdm(df_query.iterrows(), total=len(df_query), desc=\"Reranking\"):\n",
    "        query_text = row['tweet_text']\n",
    "        topk_docs = row['bm25_topk'] \n",
    "\n",
    "        pairs = [\n",
    "            [\n",
    "                query_text, \n",
    "                df_collection.loc[df_collection['cord_uid'] == doc_id, 'title'].values[0] + \" \" + \n",
    "                df_collection.loc[df_collection['cord_uid'] == doc_id, 'abstract'].values[0] + \" \" +\n",
    "                str(df_collection.loc[df_collection['cord_uid'] == doc_id, 'journal'].values[0]) + \" \" +\n",
    "                str(df_collection.loc[df_collection['cord_uid'] == doc_id, 'last_names'].values[0]) \n",
    "            ]\n",
    "            for doc_id in topk_docs\n",
    "        ]\n",
    "        \n",
    "        batch_scores = []\n",
    "        batch_docs = []\n",
    "\n",
    "        for i in range(0, len(pairs), batch_size):\n",
    "            batch_pairs = pairs[i:i + batch_size]\n",
    "            with torch.no_grad():\n",
    "                inputs = tokenizer(batch_pairs, padding=True, truncation=True, return_tensors='pt', max_length=max_length)\n",
    "                if torch.cuda.is_available():\n",
    "                    inputs = {key: value.to('cuda') for key, value in inputs.items()}\n",
    "                scores = model(**inputs, return_dict=True).logits.view(-1).float()\n",
    "\n",
    "            batch_scores.extend(scores.tolist())\n",
    "            batch_docs.extend(topk_docs[i:i+batch_size])\n",
    "\n",
    "\n",
    "        reranked = sorted(zip(batch_docs, batch_scores), key=lambda x: x[1], reverse=True)\n",
    "        reranked_results.append([doc_id for doc_id, _ in reranked[:k]])\n",
    "        reranked_scores.append([score for _, score in reranked])\n",
    "        reranked_docs.append([doc_id for doc_id, _ in reranked])\n",
    "\n",
    "    df_query['reranked_topk'] = reranked_results\n",
    "    df_query['reranked_docs'] = reranked_docs\n",
    "    df_query['reranked_scores'] = reranked_scores\n",
    "    return df_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e360937c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_mxbai(df_query, df_collection, model_name, k=10, batch_size=1):\n",
    "    reranked_results = []    \n",
    "    reranked_scores = []\n",
    "\n",
    "    model = MxbaiRerankV2(model_name, torch_dtype=torch.float16)\n",
    "\n",
    "    for _, row in tqdm(df_query.iterrows(), total=len(df_query), desc=\"Reranking\"):\n",
    "        query_text = row['tweet_text']\n",
    "        topk_docs = row['bm25_topk']  # Get top-k BM25 results for the query\n",
    "\n",
    "        documents = [\n",
    "            [\n",
    "                df_collection.loc[df_collection['cord_uid'] == doc_id, 'title'].values[0] + \" \" + \n",
    "                df_collection.loc[df_collection['cord_uid'] == doc_id, 'abstract'].values[0] + \" \" +\n",
    "                str(df_collection.loc[df_collection['cord_uid'] == doc_id, 'journal'].values[0]) + \" \" +\n",
    "                str(df_collection.loc[df_collection['cord_uid'] == doc_id, 'last_names'].values[0]) \n",
    "            ]\n",
    "            for doc_id in topk_docs\n",
    "        ]\n",
    "\n",
    "        results = model.rank(query_text, documents, return_documents=True, top_k=k, batch_size=batch_size)\n",
    "        reranked = [topk_docs[result.index] for result in results]\n",
    "        reranked_score = [result.score for result in results]\n",
    "\n",
    "        reranked_results.append(reranked)\n",
    "        reranked_scores.append(reranked_score)\n",
    "\n",
    "    df_query['reranked_topk'] = reranked_results\n",
    "    df_query['reranked_docs'] = reranked_results\n",
    "    df_query['reranked_scores'] = reranked_scores\n",
    "    \n",
    "    return df_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f96d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_reranked_results(df_query, col_gold='cord_uid', col_pred='reranked_topk', list_k=[1, 5, 10]):\n",
    "    return get_performance_mrr(df_query, col_gold, col_pred, list_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0ec103",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee16e593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reranking: 100%|██████████| 14253/14253 [2:04:03<00:00,  1.91it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1\n",
      "Number of queries in top 1: 9658\n",
      "Number of queries not in top 1: 4595\n",
      "k = 5\n",
      "Number of queries in top 5: 11240\n",
      "Number of queries not in top 5: 3013\n",
      "k = 10\n",
      "Number of queries in top 10: 11565\n",
      "Number of queries not in top 10: 2688\n",
      "Evaluation results: {1: np.float64(0.6776117308636779), 5: np.float64(0.7225660094950771), 10: np.float64(0.7256685030943082)}\n"
     ]
    }
   ],
   "source": [
    "df_query_single = experiment_single_bm25(df_collection, df_query_combined, stemmer=stemmer, k=50)\n",
    "model_name = \"Alibaba-NLP/gte-reranker-modernbert-base\"\n",
    "tokenizer, model = initialize_reranker(model_name)\n",
    "df_query = rerank_with_alibaba(df_query_single, df_collection, tokenizer, model, k=10, batch_size=100)\n",
    "df_query.to_parquet('data/reranked_results_alibaba.parquet')\n",
    "results = evaluate_reranked_results(df_query)\n",
    "print(f\"Evaluation results: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ed1b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reranking: 100%|██████████| 14253/14253 [1:49:21<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1\n",
      "Number of queries in top 1: 9026\n",
      "Number of queries not in top 1: 5227\n",
      "k = 5\n",
      "Number of queries in top 5: 10949\n",
      "Number of queries not in top 5: 3304\n",
      "k = 10\n",
      "Number of queries in top 10: 11361\n",
      "Number of queries not in top 10: 2892\n",
      "Evaluation results: {1: np.float64(0.6332701887321968), 5: np.float64(0.6880668397296476), 10: np.float64(0.6920471490825101)}\n"
     ]
    }
   ],
   "source": [
    "df_query_dev_single = experiment_single_bm25(df_collection, df_query_combined, stemmer=stemmer, k=50)\n",
    "model_name = \"Alibaba-NLP/gte-multilingual-reranker-base\"\n",
    "tokenizer, model = initialize_reranker(model_name)\n",
    "df_query = rerank_with_alibaba(df_query_dev_single, df_collection, tokenizer, model, k=10, batch_size=100)\n",
    "df_query.to_parquet('data/reranked_results_alibaba_multilingual.parquet')\n",
    "results = evaluate_reranked_results(df_query)\n",
    "print(f\"Evaluation results: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c75753",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>bm25_topk</th>\n",
       "      <th>reranked_topk</th>\n",
       "      <th>reranked_docs</th>\n",
       "      <th>reranked_scores</th>\n",
       "      <th>in_topx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>covid recovery: this study from the usa reveal...</td>\n",
       "      <td>3qvh482o</td>\n",
       "      <td>[atji1xge, mb18fj8a, 66g5lpm6, 59up4v56, gatxu...</td>\n",
       "      <td>[hg3xpej0, 59up4v56, 82y56t7d, 86xwnpde, 8t2ti...</td>\n",
       "      <td>[hg3xpej0, 59up4v56, 82y56t7d, 86xwnpde, 8t2ti...</td>\n",
       "      <td>[1.2587890625, 0.71435546875, 0.4404296875, 0....</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69</td>\n",
       "      <td>\"Among 139 clients exposed to two symptomatic ...</td>\n",
       "      <td>r58aohnu</td>\n",
       "      <td>[r58aohnu, p0kg6dyz, yrowv62k, s2vckt2w, j1ucr...</td>\n",
       "      <td>[r58aohnu, kiq6xb6k, s2vckt2w, icgsbelo, eay6q...</td>\n",
       "      <td>[r58aohnu, kiq6xb6k, s2vckt2w, icgsbelo, eay6q...</td>\n",
       "      <td>[3.333984375, 0.349853515625, 0.17138671875, 0...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73</td>\n",
       "      <td>I recall early on reading that researchers who...</td>\n",
       "      <td>sts48u9i</td>\n",
       "      <td>[sgo76prc, sts48u9i, tz2shoso, gruir7aw, 3xw4q...</td>\n",
       "      <td>[sts48u9i, o47v5vgw, a7frertc, o877uul1, u5nxm...</td>\n",
       "      <td>[sts48u9i, o47v5vgw, a7frertc, o877uul1, u5nxm...</td>\n",
       "      <td>[0.309814453125, -0.06854248046875, -0.1201171...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93</td>\n",
       "      <td>You know you're credible when NIH website has ...</td>\n",
       "      <td>3sr2exq9</td>\n",
       "      <td>[3sr2exq9, hgpiig0g, sv48gjkk, 1cpjqav4, k0f4c...</td>\n",
       "      <td>[3sr2exq9, k0f4cwig, sv48gjkk, 8j3bb6zx, pq3n1...</td>\n",
       "      <td>[3sr2exq9, k0f4cwig, sv48gjkk, 8j3bb6zx, pq3n1...</td>\n",
       "      <td>[0.83642578125, 0.22509765625, 0.1030883789062...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>96</td>\n",
       "      <td>Resistance to antifungal medications is a grow...</td>\n",
       "      <td>ybwwmyqy</td>\n",
       "      <td>[3l6ipiwk, vabb2f26, ouvq2wpq, lzddnb8j, ybwwm...</td>\n",
       "      <td>[ybwwmyqy, rs3umc1x, ouvq2wpq, 3l6ipiwk, ierqf...</td>\n",
       "      <td>[ybwwmyqy, rs3umc1x, ouvq2wpq, 3l6ipiwk, ierqf...</td>\n",
       "      <td>[0.22314453125, 0.08758544921875, -0.127197265...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id                                         tweet_text  cord_uid  \\\n",
       "0       16  covid recovery: this study from the usa reveal...  3qvh482o   \n",
       "1       69  \"Among 139 clients exposed to two symptomatic ...  r58aohnu   \n",
       "2       73  I recall early on reading that researchers who...  sts48u9i   \n",
       "3       93  You know you're credible when NIH website has ...  3sr2exq9   \n",
       "4       96  Resistance to antifungal medications is a grow...  ybwwmyqy   \n",
       "\n",
       "                                           bm25_topk  \\\n",
       "0  [atji1xge, mb18fj8a, 66g5lpm6, 59up4v56, gatxu...   \n",
       "1  [r58aohnu, p0kg6dyz, yrowv62k, s2vckt2w, j1ucr...   \n",
       "2  [sgo76prc, sts48u9i, tz2shoso, gruir7aw, 3xw4q...   \n",
       "3  [3sr2exq9, hgpiig0g, sv48gjkk, 1cpjqav4, k0f4c...   \n",
       "4  [3l6ipiwk, vabb2f26, ouvq2wpq, lzddnb8j, ybwwm...   \n",
       "\n",
       "                                       reranked_topk  \\\n",
       "0  [hg3xpej0, 59up4v56, 82y56t7d, 86xwnpde, 8t2ti...   \n",
       "1  [r58aohnu, kiq6xb6k, s2vckt2w, icgsbelo, eay6q...   \n",
       "2  [sts48u9i, o47v5vgw, a7frertc, o877uul1, u5nxm...   \n",
       "3  [3sr2exq9, k0f4cwig, sv48gjkk, 8j3bb6zx, pq3n1...   \n",
       "4  [ybwwmyqy, rs3umc1x, ouvq2wpq, 3l6ipiwk, ierqf...   \n",
       "\n",
       "                                       reranked_docs  \\\n",
       "0  [hg3xpej0, 59up4v56, 82y56t7d, 86xwnpde, 8t2ti...   \n",
       "1  [r58aohnu, kiq6xb6k, s2vckt2w, icgsbelo, eay6q...   \n",
       "2  [sts48u9i, o47v5vgw, a7frertc, o877uul1, u5nxm...   \n",
       "3  [3sr2exq9, k0f4cwig, sv48gjkk, 8j3bb6zx, pq3n1...   \n",
       "4  [ybwwmyqy, rs3umc1x, ouvq2wpq, 3l6ipiwk, ierqf...   \n",
       "\n",
       "                                     reranked_scores  in_topx  \n",
       "0  [1.2587890625, 0.71435546875, 0.4404296875, 0....      0.0  \n",
       "1  [3.333984375, 0.349853515625, 0.17138671875, 0...      1.0  \n",
       "2  [0.309814453125, -0.06854248046875, -0.1201171...      1.0  \n",
       "3  [0.83642578125, 0.22509765625, 0.1030883789062...      1.0  \n",
       "4  [0.22314453125, 0.08758544921875, -0.127197265...      1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reranking: 100%|██████████| 14253/14253 [1:30:21<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1\n",
      "Number of queries in top 1: 7438\n",
      "Number of queries not in top 1: 6815\n",
      "k = 5\n",
      "Number of queries in top 5: 9521\n",
      "Number of queries not in top 5: 4732\n",
      "k = 10\n",
      "Number of queries in top 10: 10215\n",
      "Number of queries not in top 10: 4038\n",
      "Evaluation results: {1: np.float64(0.5218550480600576), 5: np.float64(0.5785320049580205), 10: np.float64(0.5850188876972712)}\n"
     ]
    }
   ],
   "source": [
    "df_query_dev_single = experiment_single_bm25(df_collection, df_query_combined, stemmer=stemmer, k=200)\n",
    "display(df_query_dev_single.head(5))\n",
    "model_name = \"cross-encoder/ms-marco-TinyBERT-L2-v2\"\n",
    "tokenizer, model = initialize_reranker(model_name)\n",
    "df_query = rerank_with_alibaba(df_query_dev_single, df_collection, tokenizer, model, k=10, batch_size=100)\n",
    "df_query.to_parquet(\"data/reranked_results_tinyBERT.parquet\")\n",
    "results = evaluate_reranked_results(df_query)\n",
    "print(f\"Evaluation results: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99af320",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>bm25_topk</th>\n",
       "      <th>reranked_topk</th>\n",
       "      <th>reranked_docs</th>\n",
       "      <th>reranked_scores</th>\n",
       "      <th>in_topx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>covid recovery: this study from the usa reveal...</td>\n",
       "      <td>3qvh482o</td>\n",
       "      <td>[atji1xge, mb18fj8a, 66g5lpm6, 59up4v56, gatxu...</td>\n",
       "      <td>[nksd3wuw, es8l29ub, atji1xge, 82y56t7d, sqxdw...</td>\n",
       "      <td>[nksd3wuw, es8l29ub, atji1xge, 82y56t7d, sqxdw...</td>\n",
       "      <td>[7.078125, 6.046875, 5.74609375, 5.4140625, 5....</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69</td>\n",
       "      <td>\"Among 139 clients exposed to two symptomatic ...</td>\n",
       "      <td>r58aohnu</td>\n",
       "      <td>[r58aohnu, p0kg6dyz, yrowv62k, s2vckt2w, j1ucr...</td>\n",
       "      <td>[r58aohnu, 6zfpcm4j, wk61uyrt, xsqgrd5l, icgsb...</td>\n",
       "      <td>[r58aohnu, 6zfpcm4j, wk61uyrt, xsqgrd5l, icgsb...</td>\n",
       "      <td>[6.75390625, 1.7626953125, 1.494140625, 1.3847...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73</td>\n",
       "      <td>I recall early on reading that researchers who...</td>\n",
       "      <td>sts48u9i</td>\n",
       "      <td>[sgo76prc, sts48u9i, tz2shoso, gruir7aw, 3xw4q...</td>\n",
       "      <td>[u5nxm9tu, qkg8fwbp, sts48u9i, 4aps0kvp, ujq9m...</td>\n",
       "      <td>[u5nxm9tu, qkg8fwbp, sts48u9i, 4aps0kvp, ujq9m...</td>\n",
       "      <td>[0.5537109375, -0.328857421875, -0.48095703125...</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93</td>\n",
       "      <td>You know you're credible when NIH website has ...</td>\n",
       "      <td>3sr2exq9</td>\n",
       "      <td>[3sr2exq9, hgpiig0g, sv48gjkk, 1cpjqav4, k0f4c...</td>\n",
       "      <td>[kdegnr6i, bn22k0p3, 3sr2exq9, k0f4cwig, wbw7g...</td>\n",
       "      <td>[kdegnr6i, bn22k0p3, 3sr2exq9, k0f4cwig, wbw7g...</td>\n",
       "      <td>[-0.0164794921875, -1.2412109375, -1.251953125...</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>96</td>\n",
       "      <td>Resistance to antifungal medications is a grow...</td>\n",
       "      <td>ybwwmyqy</td>\n",
       "      <td>[3l6ipiwk, vabb2f26, ouvq2wpq, lzddnb8j, ybwwm...</td>\n",
       "      <td>[vabb2f26, ybwwmyqy, 3l6ipiwk, ouvq2wpq, buswb...</td>\n",
       "      <td>[vabb2f26, ybwwmyqy, 3l6ipiwk, ouvq2wpq, buswb...</td>\n",
       "      <td>[4.51171875, 4.36328125, 4.16015625, 2.578125,...</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id                                         tweet_text  cord_uid  \\\n",
       "0       16  covid recovery: this study from the usa reveal...  3qvh482o   \n",
       "1       69  \"Among 139 clients exposed to two symptomatic ...  r58aohnu   \n",
       "2       73  I recall early on reading that researchers who...  sts48u9i   \n",
       "3       93  You know you're credible when NIH website has ...  3sr2exq9   \n",
       "4       96  Resistance to antifungal medications is a grow...  ybwwmyqy   \n",
       "\n",
       "                                           bm25_topk  \\\n",
       "0  [atji1xge, mb18fj8a, 66g5lpm6, 59up4v56, gatxu...   \n",
       "1  [r58aohnu, p0kg6dyz, yrowv62k, s2vckt2w, j1ucr...   \n",
       "2  [sgo76prc, sts48u9i, tz2shoso, gruir7aw, 3xw4q...   \n",
       "3  [3sr2exq9, hgpiig0g, sv48gjkk, 1cpjqav4, k0f4c...   \n",
       "4  [3l6ipiwk, vabb2f26, ouvq2wpq, lzddnb8j, ybwwm...   \n",
       "\n",
       "                                       reranked_topk  \\\n",
       "0  [nksd3wuw, es8l29ub, atji1xge, 82y56t7d, sqxdw...   \n",
       "1  [r58aohnu, 6zfpcm4j, wk61uyrt, xsqgrd5l, icgsb...   \n",
       "2  [u5nxm9tu, qkg8fwbp, sts48u9i, 4aps0kvp, ujq9m...   \n",
       "3  [kdegnr6i, bn22k0p3, 3sr2exq9, k0f4cwig, wbw7g...   \n",
       "4  [vabb2f26, ybwwmyqy, 3l6ipiwk, ouvq2wpq, buswb...   \n",
       "\n",
       "                                       reranked_docs  \\\n",
       "0  [nksd3wuw, es8l29ub, atji1xge, 82y56t7d, sqxdw...   \n",
       "1  [r58aohnu, 6zfpcm4j, wk61uyrt, xsqgrd5l, icgsb...   \n",
       "2  [u5nxm9tu, qkg8fwbp, sts48u9i, 4aps0kvp, ujq9m...   \n",
       "3  [kdegnr6i, bn22k0p3, 3sr2exq9, k0f4cwig, wbw7g...   \n",
       "4  [vabb2f26, ybwwmyqy, 3l6ipiwk, ouvq2wpq, buswb...   \n",
       "\n",
       "                                     reranked_scores   in_topx  \n",
       "0  [7.078125, 6.046875, 5.74609375, 5.4140625, 5....  0.000000  \n",
       "1  [6.75390625, 1.7626953125, 1.494140625, 1.3847...  1.000000  \n",
       "2  [0.5537109375, -0.328857421875, -0.48095703125...  0.333333  \n",
       "3  [-0.0164794921875, -1.2412109375, -1.251953125...  0.333333  \n",
       "4  [4.51171875, 4.36328125, 4.16015625, 2.578125,...  0.500000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reranking: 100%|██████████| 14253/14253 [1:30:00<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1\n",
      "Number of queries in top 1: 7613\n",
      "Number of queries not in top 1: 6640\n",
      "k = 5\n",
      "Number of queries in top 5: 9648\n",
      "Number of queries not in top 5: 4605\n",
      "k = 10\n",
      "Number of queries in top 10: 10355\n",
      "Number of queries not in top 10: 3898\n",
      "Evaluation results: {1: np.float64(0.5341331649477303), 5: np.float64(0.5887462288640988), 10: np.float64(0.5954533659858855)}\n"
     ]
    }
   ],
   "source": [
    "df_query_dev_single = experiment_single_bm25(df_collection, df_query_combined, stemmer=stemmer, k=100)\n",
    "display(df_query_dev_single.head(5))\n",
    "model_name = \"cross-encoder/ms-marco-MiniLM-L12-v2\"\n",
    "tokenizer, model = initialize_reranker(model_name)\n",
    "df_query = rerank_with_alibaba(df_query_dev_single, df_collection, tokenizer, model, k=10, batch_size=100)\n",
    "df_query.to_parquet(\"data/reranked_results_miniLM.parquet\")\n",
    "results = evaluate_reranked_results(df_query)\n",
    "print(f\"Evaluation results: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069665d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reranking: 100%|██████████| 14253/14253 [1:20:58<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1\n",
      "Number of queries in top 1: 7032\n",
      "Number of queries not in top 1: 7221\n",
      "k = 5\n",
      "Number of queries in top 5: 9334\n",
      "Number of queries not in top 5: 4919\n",
      "k = 10\n",
      "Number of queries in top 10: 10228\n",
      "Number of queries not in top 10: 4025\n",
      "Evaluation results: {1: np.float64(0.49336981688065673), 5: np.float64(0.5558163193713604), 10: np.float64(0.5641402422659)}\n"
     ]
    }
   ],
   "source": [
    "df_query_dev_single = experiment_single_bm25(df_collection, df_query_combined, stemmer=stemmer, k=40)\n",
    "model_name = \"cross-encoder/ms-marco-electra-base\"\n",
    "tokenizer, model = initialize_reranker(model_name)\n",
    "df_query = rerank_with_alibaba(df_query_dev_single, df_collection, tokenizer, model, k=10, batch_size=50, max_length=None )\n",
    "df_query.to_parquet(\"data/reranked_results_electra.parquet\")\n",
    "results = evaluate_reranked_results(df_query)\n",
    "print(f\"Evaluation results: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd25c9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reranking:   0%|          | 0/14253 [00:00<?, ?it/s]                        You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Reranking: 100%|██████████| 14253/14253 [4:14:34<00:00,  1.07s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1\n",
      "Number of queries in top 1: 9784\n",
      "Number of queries not in top 1: 4469\n",
      "k = 5\n",
      "Number of queries in top 5: 11050\n",
      "Number of queries not in top 5: 3203\n",
      "k = 10\n",
      "Number of queries in top 10: 11274\n",
      "Number of queries not in top 10: 2979\n",
      "Evaluation results: {1: np.float64(0.6864519750228022), 5: np.float64(0.7231974555064431), 10: np.float64(0.7253506362904384)}\n"
     ]
    }
   ],
   "source": [
    "df_query_dev_single = experiment_single_bm25(df_collection, df_query_combined, stemmer=stemmer, k=25)\n",
    "df_query = rerank_mxbai(df_query_dev_single, df_collection, \"mixedbread-ai/mxbai-rerank-base-v2\", batch_size=1)\n",
    "df_query.to_parquet(\"data/reranked_results_mxbai.parquet\")\n",
    "results = evaluate_reranked_results(df_query)\n",
    "print(f\"Evaluation results: {results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbff983",
   "metadata": {},
   "source": [
    "## Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b1d662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from sentence_transformers import CrossEncoder\n",
    "from sentence_transformers.cross_encoder.losses import CachedMultipleNegativesRankingLoss, MultipleNegativesRankingLoss\n",
    "from sentence_transformers.cross_encoder import CrossEncoderTrainingArguments\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import mine_hard_negatives\n",
    "from sentence_transformers.cross_encoder.evaluation import CrossEncoderRerankingEvaluator\n",
    "from sentence_transformers.cross_encoder.losses.BinaryCrossEntropyLoss import BinaryCrossEntropyLoss\n",
    "from sentence_transformers.cross_encoder import CrossEncoderTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e63c5e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>source_x</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>pubmed_id</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>mag_id</th>\n",
       "      <th>who_covidence_id</th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>label</th>\n",
       "      <th>time</th>\n",
       "      <th>timet</th>\n",
       "      <th>last_names</th>\n",
       "      <th>publish_year</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>umvrwgaw</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Professional and Home-Made Face Masks Reduce E...</td>\n",
       "      <td>10.1371/journal.pone.0002618</td>\n",
       "      <td>PMC2440799</td>\n",
       "      <td>18612429</td>\n",
       "      <td>cc-by</td>\n",
       "      <td>BACKGROUND: Governments are preparing for a po...</td>\n",
       "      <td>2008-07-09</td>\n",
       "      <td>van der Sande, Marianne; Teunis, Peter; Sabel,...</td>\n",
       "      <td>PLoS One</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>umvrwgaw</td>\n",
       "      <td>2008-07-09</td>\n",
       "      <td>1215561600</td>\n",
       "      <td>van der Sande; Teunis; Sabel</td>\n",
       "      <td>2008</td>\n",
       "      <td>Professional and Home-Made Face Masks Reduce E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>spiud6ok</td>\n",
       "      <td>PMC</td>\n",
       "      <td>The Failure of R (0)</td>\n",
       "      <td>10.1155/2011/527610</td>\n",
       "      <td>PMC3157160</td>\n",
       "      <td>21860658</td>\n",
       "      <td>cc-by</td>\n",
       "      <td>The basic reproductive ratio, R (0), is one of...</td>\n",
       "      <td>2011-08-16</td>\n",
       "      <td>Li, Jing; Blakeley, Daniel; Smith?, Robert J.</td>\n",
       "      <td>Comput Math Methods Med</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>spiud6ok</td>\n",
       "      <td>2011-08-16</td>\n",
       "      <td>1313452800</td>\n",
       "      <td>Li; Blakeley; Smith?</td>\n",
       "      <td>2011</td>\n",
       "      <td>The Failure of R (0) The basic reproductive ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>aclzp3iy</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Pulmonary sequelae in a patient recovered from...</td>\n",
       "      <td>10.4103/0970-2113.99118</td>\n",
       "      <td>PMC3424870</td>\n",
       "      <td>22919170</td>\n",
       "      <td>cc-by-nc-sa</td>\n",
       "      <td>The pandemic of swine flu (H1N1) influenza spr...</td>\n",
       "      <td>2012</td>\n",
       "      <td>Singh, Virendra; Sharma, Bharat Bhushan; Patel...</td>\n",
       "      <td>Lung India</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aclzp3iy</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>1325376000</td>\n",
       "      <td>Singh; Sharma; Patel</td>\n",
       "      <td>2012</td>\n",
       "      <td>Pulmonary sequelae in a patient recovered from...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     cord_uid source_x                                              title  \\\n",
       "162  umvrwgaw      PMC  Professional and Home-Made Face Masks Reduce E...   \n",
       "611  spiud6ok      PMC                               The Failure of R (0)   \n",
       "918  aclzp3iy      PMC  Pulmonary sequelae in a patient recovered from...   \n",
       "\n",
       "                              doi       pmcid pubmed_id      license  \\\n",
       "162  10.1371/journal.pone.0002618  PMC2440799  18612429        cc-by   \n",
       "611           10.1155/2011/527610  PMC3157160  21860658        cc-by   \n",
       "918       10.4103/0970-2113.99118  PMC3424870  22919170  cc-by-nc-sa   \n",
       "\n",
       "                                              abstract publish_time  \\\n",
       "162  BACKGROUND: Governments are preparing for a po...   2008-07-09   \n",
       "611  The basic reproductive ratio, R (0), is one of...   2011-08-16   \n",
       "918  The pandemic of swine flu (H1N1) influenza spr...         2012   \n",
       "\n",
       "                                               authors  \\\n",
       "162  van der Sande, Marianne; Teunis, Peter; Sabel,...   \n",
       "611      Li, Jing; Blakeley, Daniel; Smith?, Robert J.   \n",
       "918  Singh, Virendra; Sharma, Bharat Bhushan; Patel...   \n",
       "\n",
       "                     journal  mag_id who_covidence_id arxiv_id     label  \\\n",
       "162                 PLoS One     NaN              NaN      NaN  umvrwgaw   \n",
       "611  Comput Math Methods Med     NaN              NaN      NaN  spiud6ok   \n",
       "918               Lung India     NaN              NaN      NaN  aclzp3iy   \n",
       "\n",
       "          time       timet                    last_names publish_year  \\\n",
       "162 2008-07-09  1215561600  van der Sande; Teunis; Sabel         2008   \n",
       "611 2011-08-16  1313452800          Li; Blakeley; Smith?         2011   \n",
       "918 2012-01-01  1325376000          Singh; Sharma; Patel         2012   \n",
       "\n",
       "                                                answer  \n",
       "162  Professional and Home-Made Face Masks Reduce E...  \n",
       "611  The Failure of R (0) The basic reproductive ra...  \n",
       "918  Pulmonary sequelae in a patient recovered from...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_collection['answer'] = df_collection.apply(\n",
    "    lambda row: f\"{row['title']} {row['abstract']} {row['last_names']} {row['journal']}\", axis=1\n",
    ")\n",
    "df_collection.head(3)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b460ba5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>bm25_topk</th>\n",
       "      <th>in_topx</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Oral care in rehabilitation medicine: oral vul...</td>\n",
       "      <td>htlvpvz5</td>\n",
       "      <td>[htlvpvz5, h7hj64q5, 4aps0kvp, 5tkyir3r, 32z7b...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Oral Management in Rehabilitation Medicine: Or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>this study isn't receiving sufficient attentio...</td>\n",
       "      <td>4kfl29ul</td>\n",
       "      <td>[maj8r6ti, bjvg2ivr, 7tto4hr7, 2cwvga0k, 46je8...</td>\n",
       "      <td>0.003145</td>\n",
       "      <td>Variation in racial/ethnic disparities in COVI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>thanks, xi jinping. a reminder that this study...</td>\n",
       "      <td>jtwb17u8</td>\n",
       "      <td>[jtwb17u8, veeavho5, jbpmbm9m, 8hkxbxz9, 32v44...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Effect of non-pharmaceutical interventions for...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id                                         tweet_text  cord_uid  \\\n",
       "0        0  Oral care in rehabilitation medicine: oral vul...  htlvpvz5   \n",
       "1        1  this study isn't receiving sufficient attentio...  4kfl29ul   \n",
       "2        2  thanks, xi jinping. a reminder that this study...  jtwb17u8   \n",
       "\n",
       "                                           bm25_topk   in_topx  \\\n",
       "0  [htlvpvz5, h7hj64q5, 4aps0kvp, 5tkyir3r, 32z7b...  1.000000   \n",
       "1  [maj8r6ti, bjvg2ivr, 7tto4hr7, 2cwvga0k, 46je8...  0.003145   \n",
       "2  [jtwb17u8, veeavho5, jbpmbm9m, 8hkxbxz9, 32v44...  1.000000   \n",
       "\n",
       "                                              answer  \n",
       "0  Oral Management in Rehabilitation Medicine: Or...  \n",
       "1  Variation in racial/ethnic disparities in COVI...  \n",
       "2  Effect of non-pharmaceutical interventions for...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_query_train.merge(df_collection[['cord_uid', 'answer']], on='cord_uid', how='left')\n",
    "df_dev = df_query_dev.merge(df_collection[['cord_uid', 'answer']], on='cord_uid', how='left')\n",
    "df_full = pd.concat([df_train, df_dev], ignore_index=True)\n",
    "df_full.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56627c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict({\n",
    "    \"query\": df_train['tweet_text'].tolist(),\n",
    "    \"document\": df_train['answer'].tolist(),\n",
    "})\n",
    "\n",
    "dev_dataset = Dataset.from_dict({\n",
    "    \"query\": df_dev['tweet_text'].tolist(),\n",
    "    \"document\": df_dev['answer'].tolist(),\n",
    "})\n",
    "full_dataset = Dataset.from_dict({\n",
    "    \"query\": df_full['tweet_text'].tolist(),\n",
    "    \"document\": df_full['answer'].tolist(),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3388f98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hard_negatives(embedding_model_hard_negatives, num_hard_negatives, num_hard_negatives_eval, train_dataset, dev_dataset, df_full, batch_size=4096):\n",
    "\n",
    "    embedding_model = SentenceTransformer(embedding_model_hard_negatives, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "    # hard_train_dataset = mine_hard_negatives(\n",
    "    #     train_dataset,\n",
    "    #     embedding_model,\n",
    "    #     num_negatives=num_hard_negatives,  # How many negatives per question-answer pair\n",
    "    #     range_min=3,  # Skip the x most similar samples\n",
    "    #     range_max=100,  # Consider only the x most similar samples\n",
    "    #     max_score=0.95,  # Only consider samples with a similarity score of at most x\n",
    "    #     margin=0.05,  # Similarity between query and negative samples should be x lower than query-positive similarity\n",
    "    #     sampling_strategy=\"top\",  # Randomly sample negatives from the range\n",
    "    #     batch_size=batch_size,  # Use a batch size of 4096 for the embedding model\n",
    "    #     output_format=\"labeled-pair\",  # The output format is (query, passage, label), as required by BinaryCrossEntropyLoss\n",
    "    # )\n",
    "\n",
    "    # hard_eval_dataset = mine_hard_negatives(\n",
    "    #     dev_dataset,\n",
    "    #     embedding_model,\n",
    "    #     corpus=df_full[\"answer\"],  # Use the full dataset as the corpus\n",
    "    #     num_negatives=num_hard_negatives_eval,  # How many negatives per question-answer pair\n",
    "    #     batch_size=batch_size,  # Use a batch size of 4096 for the embedding model\n",
    "    #     output_format=\"n-tuple\",  # The output format is (query, positive, negative1, negative2, ...) for the evaluator\n",
    "    #     include_positives=True,  # Key: Include the positive answer in the list of negatives\n",
    "    #     range_min=3,  # Skip the x most similar samples\n",
    "    #     range_max=100,  # Consider only the x most similar samples\n",
    "    #     max_score=0.95,  # Only consider samples with a similarity score of at most x\n",
    "    #     margin=0.05,  # Similarity between query and negative samples should be x lower than query-positive similarity\n",
    "    #     sampling_strategy=\"top\",  # Randomly sample negatives from the range\n",
    "    # )\n",
    "\n",
    "    hard_train_dataset = mine_hard_negatives(\n",
    "        train_dataset,\n",
    "        embedding_model,\n",
    "        num_negatives=num_hard_negatives,  # How many negatives per question-answer pair\n",
    "        range_min=5,  # Skip the x most similar samples\n",
    "        range_max=100,  # Consider only the x most similar samples\n",
    "        max_score=0.8,  # Only consider samples with a similarity score of at most x\n",
    "        margin=0.05,  # Similarity between query and negative samples should be x lower than query-positive similarity\n",
    "        sampling_strategy=\"top\",  # Randomly sample negatives from the range\n",
    "        batch_size=batch_size,  # Use a batch size of 4096 for the embedding model\n",
    "        output_format=\"labeled-pair\",  # The output format is (query, passage, label), as required by BinaryCrossEntropyLoss\n",
    "    )\n",
    "\n",
    "    hard_eval_dataset = mine_hard_negatives(\n",
    "        dev_dataset,\n",
    "        embedding_model,\n",
    "        corpus=df_full[\"answer\"],  # Use the full dataset as the corpus\n",
    "        num_negatives=num_hard_negatives_eval,  # How many negatives per question-answer pair\n",
    "        batch_size=batch_size,  # Use a batch size of 4096 for the embedding model\n",
    "        output_format=\"n-tuple\",  # The output format is (query, positive, negative1, negative2, ...) for the evaluator\n",
    "        include_positives=True,  # Key: Include the positive answer in the list of negatives\n",
    "        # range_min=5,  # Skip the x most similar samples\n",
    "        # range_max=100,  # Consider only the x most similar samples\n",
    "        # max_score=0.8,  # Only consider samples with a similarity score of at most x\n",
    "        # margin=0.05,  # Similarity between query and negative samples should be x lower than query-positive similarity\n",
    "        # sampling_strategy=\"top\",  # Randomly sample negatives from the range\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    return hard_train_dataset, hard_eval_dataset\n",
    "\n",
    "\n",
    "class ExperimentRunner:\n",
    "    def __init__(self, model_to_finetune, train_batch_size, num_epochs, max_length, experiment_name):\n",
    "        self.experiment_name = experiment_name\n",
    "        self.model_to_finetune = model_to_finetune\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.max_length = max_length\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def run_experiment(self, dev_dataset, hard_train_dataset, hard_eval_dataset, num_hard_negatives, trust_remote_code=False, eval_steps=None):\n",
    "\n",
    "        if eval_steps is None:\n",
    "            eval_steps = 20000/self.train_batch_size\n",
    "        \n",
    "        model = CrossEncoder(self.model_to_finetune, max_length=self.max_length, device=self.device, trust_remote_code=trust_remote_code)\n",
    "\n",
    "        args = CrossEncoderTrainingArguments(\n",
    "            # Required parameter:\n",
    "            output_dir=f\"model/{self.model_to_finetune.split(\"/\")[-1]}{self.experiment_name}\",\n",
    "            # Optional training parameters:\n",
    "            num_train_epochs=self.num_epochs,\n",
    "            per_device_train_batch_size=self.train_batch_size,\n",
    "            per_device_eval_batch_size=self.train_batch_size,\n",
    "            learning_rate=2e-5,\n",
    "            warmup_ratio=0.1,\n",
    "            fp16=True,  \n",
    "            bf16=False, \n",
    "            batch_sampler=BatchSamplers.NO_DUPLICATES, \n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=eval_steps,\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=eval_steps,\n",
    "            save_total_limit=10,\n",
    "            logging_steps=eval_steps,\n",
    "            run_name=self.model_to_finetune.split(\"/\")[-1],\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "        )\n",
    "        \n",
    "        reranking_evaluator = CrossEncoderRerankingEvaluator(\n",
    "            samples=[\n",
    "                {\n",
    "                    \"query\": sample[\"query\"],\n",
    "                    \"positive\": [sample[\"document\"]],\n",
    "                    \"documents\": [sample[column_name] for column_name in hard_eval_dataset.column_names if 'negative' in column_name],\n",
    "                }\n",
    "                for sample in hard_eval_dataset\n",
    "            ],\n",
    "            batch_size=self.train_batch_size,\n",
    "            name=\"dev_set\",\n",
    "            show_progress_bar=True,\n",
    "        )\n",
    "\n",
    "        # loss = BinaryCrossEntropyLoss(model=model, pos_weight=torch.tensor(self.num_hard_negatives))\n",
    "        loss = MultipleNegativesRankingLoss(model=model, num_negatives=num_hard_negatives)\n",
    "\n",
    "        trainer = CrossEncoderTrainer(\n",
    "            model=model,\n",
    "            args=args,\n",
    "            train_dataset=hard_train_dataset,\n",
    "            eval_dataset=dev_dataset,\n",
    "            loss=loss,\n",
    "            evaluator=reranking_evaluator,\n",
    "    \n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "\n",
    "        model.save(f\"models/{self.model_to_finetune.split('/')[-1]}-finetuned{self.experiment_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "afccf474",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "# embedding_model_hard_negatives = \"sentence-transformers/static-retrieval-mrl-en-v1\"\n",
    "# embedding_model_hard_negatives = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# embedding_model_hard_negatives = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
    "embedding_model_hard_negatives = \"BAAI/bge-small-en-v1.5\"\n",
    "\n",
    "\n",
    "experiment_name = f\"_{embedding_model_hard_negatives.split('/')[-1]}_epochs{num_epochs}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "23062379",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `margin` parameter is deprecated. Use the `absolute_margin` and/or `relative_margin` parameter instead. Setting `absolute_margin` to `0.05`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12842 unique queries out of 12853 total queries.\n",
      "Found an average of 1.001 positives per query.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 109/109 [00:26<00:00,  4.04it/s]\n",
      "Batches: 100%|██████████| 201/201 [00:06<00:00, 30.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric       Positive       Negative     Difference\n",
      "Count          12,853         39,139               \n",
      "Mean           0.8195         0.7622         0.0930\n",
      "Median         0.8329         0.7814         0.0832\n",
      "Std            0.0826         0.0433         0.0395\n",
      "Min            0.3809         0.5027        -0.0891\n",
      "25%            0.7696         0.7405         0.0584\n",
      "50%            0.8329         0.7814         0.0832\n",
      "75%            0.8834         0.7947         0.1173\n",
      "Max            0.9881         0.7998         0.3004\n",
      "Skipped 561,368 potential negatives (42.44%) due to the absolute_margin of 0.05.\n",
      "Skipped 146,729 potential negatives (19.27%) due to the max_score of 0.8.\n",
      "Could not find enough negatives for 25126 samples (39.10%). Consider adjusting the range_max, range_min, absolute_margin and max_score parameters if you'd like to find more valid negatives.\n",
      "Setting range_max to 7 based on the provided parameters.\n",
      "Found 1399 unique queries out of 1400 total queries.\n",
      "Found an average of 1.001 positives per query.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 121/121 [00:29<00:00,  4.04it/s]\n",
      "Batches: 100%|██████████| 22/22 [00:00<00:00, 31.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric       Positive       Negative     Difference\n",
      "Count           1,400          7,000               \n",
      "Mean           0.8219         0.8117         0.0102\n",
      "Median         0.8323         0.8156         0.0000\n",
      "Std            0.0783         0.0543         0.0593\n",
      "Min            0.4977         0.5593        -0.1856\n",
      "25%            0.7717         0.7790        -0.0220\n",
      "50%            0.8323         0.8156         0.0000\n",
      "75%            0.8846         0.8466         0.0443\n",
      "Max            0.9681         0.9681         0.2377\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_hard_negatives = 5\n",
    "hard_train_dataset, hard_eval_dataset = get_hard_negatives(embedding_model_hard_negatives, num_hard_negatives=num_hard_negatives, num_hard_negatives_eval=5, train_dataset=train_dataset, dev_dataset=dev_dataset, df_full=df_full, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a902b443",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0b31039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (634 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3250' max='3250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3250/3250 10:42, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Dev Set Map</th>\n",
       "      <th>Dev Set Mrr@10</th>\n",
       "      <th>Dev Set Ndcg@10</th>\n",
       "      <th>Dev Set Base Map</th>\n",
       "      <th>Dev Set Base Mrr@10</th>\n",
       "      <th>Dev Set Base Ndcg@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>1.569700</td>\n",
       "      <td>0.232398</td>\n",
       "      <td>0.760155</td>\n",
       "      <td>0.760155</td>\n",
       "      <td>0.818259</td>\n",
       "      <td>0.568988</td>\n",
       "      <td>0.568988</td>\n",
       "      <td>0.595057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.508500</td>\n",
       "      <td>0.229276</td>\n",
       "      <td>0.767536</td>\n",
       "      <td>0.767536</td>\n",
       "      <td>0.823919</td>\n",
       "      <td>0.568988</td>\n",
       "      <td>0.568988</td>\n",
       "      <td>0.595057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1875</td>\n",
       "      <td>1.496600</td>\n",
       "      <td>0.191326</td>\n",
       "      <td>0.770571</td>\n",
       "      <td>0.770571</td>\n",
       "      <td>0.826231</td>\n",
       "      <td>0.568988</td>\n",
       "      <td>0.568988</td>\n",
       "      <td>0.595057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.484700</td>\n",
       "      <td>0.216610</td>\n",
       "      <td>0.773869</td>\n",
       "      <td>0.773869</td>\n",
       "      <td>0.828695</td>\n",
       "      <td>0.568988</td>\n",
       "      <td>0.568988</td>\n",
       "      <td>0.595057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3125</td>\n",
       "      <td>1.485200</td>\n",
       "      <td>0.211380</td>\n",
       "      <td>0.772512</td>\n",
       "      <td>0.772512</td>\n",
       "      <td>0.827716</td>\n",
       "      <td>0.568988</td>\n",
       "      <td>0.568988</td>\n",
       "      <td>0.595057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    }
   ],
   "source": [
    "train_batch_size = 32\n",
    "max_length = 512\n",
    "model_to_finetune = \"cross-encoder/ms-marco-TinyBERT-L2-v2\"\n",
    "\n",
    "exp_runner = ExperimentRunner(\n",
    "    model_to_finetune,\n",
    "    train_batch_size,\n",
    "    num_epochs,\n",
    "    max_length,\n",
    "    experiment_name=experiment_name\n",
    ")\n",
    "exp_runner.run_experiment(dev_dataset, hard_train_dataset, hard_eval_dataset, num_hard_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d051f523",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 4\n",
    "max_length = 512\n",
    "model_to_finetune = \"cross-encoder/ms-marco-MiniLM-L12-v2\"\n",
    "\n",
    "exp_runner = ExperimentRunner(\n",
    "    model_to_finetune,\n",
    "    train_batch_size,\n",
    "    num_epochs,\n",
    "    max_length,\n",
    "    experiment_name=experiment_name\n",
    ")\n",
    "exp_runner.run_experiment(dev_dataset, hard_train_dataset, hard_eval_dataset, num_hard_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dcc1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `margin` parameter is deprecated. Use the `absolute_margin` and/or `relative_margin` parameter instead. Setting `absolute_margin` to `0.05`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12842 unique queries out of 12853 total queries.\n",
      "Found an average of 1.001 positives per query.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  3.65it/s]\n",
      "Batches: 100%|██████████| 4/4 [00:00<00:00, 26.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric       Positive       Negative     Difference\n",
      "Count          12,853         45,823               \n",
      "Mean           0.5263         0.4702         0.1262\n",
      "Median         0.5440         0.4759         0.0969\n",
      "Std            0.1639         0.1055         0.0809\n",
      "Min           -0.0783         0.1255        -0.2308\n",
      "25%            0.4171         0.3980         0.0648\n",
      "50%            0.5440         0.4759         0.0969\n",
      "75%            0.6514         0.5476         0.1625\n",
      "Max            0.9287         0.7643         0.6667\n",
      "Skipped 493,639 potential negatives (37.32%) due to the absolute_margin of 0.05.\n",
      "Skipped 5 potential negatives (0.00%) due to the max_score of 0.8.\n",
      "Could not find enough negatives for 18442 samples (28.70%). Consider adjusting the range_max, range_min, absolute_margin and max_score parameters if you'd like to find more valid negatives.\n",
      "Setting range_max to 7 based on the provided parameters.\n",
      "Found 1399 unique queries out of 1400 total queries.\n",
      "Found an average of 1.001 positives per query.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  3.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 55.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric       Positive       Negative     Difference\n",
      "Count           1,400          7,000               \n",
      "Mean           0.5248         0.5402        -0.0154\n",
      "Median         0.5435         0.5493        -0.0000\n",
      "Std            0.1603         0.1151         0.1361\n",
      "Min            0.0295         0.1618        -0.6029\n",
      "25%            0.4099         0.4616        -0.0925\n",
      "50%            0.5438         0.5493        -0.0000\n",
      "75%            0.6468         0.6234         0.0614\n",
      "Max            0.8905         0.8905         0.5734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (624 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14658' max='14658' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14658/14658 1:01:41, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Dev Set Map</th>\n",
       "      <th>Dev Set Mrr@10</th>\n",
       "      <th>Dev Set Ndcg@10</th>\n",
       "      <th>Dev Set Base Map</th>\n",
       "      <th>Dev Set Base Mrr@10</th>\n",
       "      <th>Dev Set Base Ndcg@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.492300</td>\n",
       "      <td>0.175127</td>\n",
       "      <td>0.808929</td>\n",
       "      <td>0.808929</td>\n",
       "      <td>0.855300</td>\n",
       "      <td>0.465655</td>\n",
       "      <td>0.465655</td>\n",
       "      <td>0.491332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.485600</td>\n",
       "      <td>0.313195</td>\n",
       "      <td>0.822571</td>\n",
       "      <td>0.822571</td>\n",
       "      <td>0.865500</td>\n",
       "      <td>0.465655</td>\n",
       "      <td>0.465655</td>\n",
       "      <td>0.491332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.488300</td>\n",
       "      <td>0.253402</td>\n",
       "      <td>0.827405</td>\n",
       "      <td>0.827405</td>\n",
       "      <td>0.869210</td>\n",
       "      <td>0.465655</td>\n",
       "      <td>0.465655</td>\n",
       "      <td>0.491332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.471900</td>\n",
       "      <td>0.196620</td>\n",
       "      <td>0.834333</td>\n",
       "      <td>0.834333</td>\n",
       "      <td>0.874425</td>\n",
       "      <td>0.465655</td>\n",
       "      <td>0.465655</td>\n",
       "      <td>0.491332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>1.440500</td>\n",
       "      <td>0.217848</td>\n",
       "      <td>0.838333</td>\n",
       "      <td>0.838333</td>\n",
       "      <td>0.877416</td>\n",
       "      <td>0.465655</td>\n",
       "      <td>0.465655</td>\n",
       "      <td>0.491332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    }
   ],
   "source": [
    "train_batch_size = 8\n",
    "max_length = 512\n",
    "model_to_finetune = \"cross-encoder/ms-marco-MiniLM-L6-v2\"\n",
    "\n",
    "exp_runner = ExperimentRunner(\n",
    "    model_to_finetune,\n",
    "    train_batch_size,\n",
    "    num_epochs,\n",
    "    max_length,\n",
    "    experiment_name=experiment_name\n",
    ")\n",
    "exp_runner.run_experiment(dev_dataset, hard_train_dataset, hard_eval_dataset, num_hard_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8f9fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `margin` parameter is deprecated. Use the `absolute_margin` and/or `relative_margin` parameter instead. Setting `absolute_margin` to `0.05`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12842 unique queries out of 12853 total queries.\n",
      "Found an average of 1.001 positives per query.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  3.73it/s]\n",
      "Batches: 100%|██████████| 4/4 [00:00<00:00, 28.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric       Positive       Negative     Difference\n",
      "Count          12,853         45,823               \n",
      "Mean           0.5263         0.4702         0.1262\n",
      "Median         0.5440         0.4759         0.0969\n",
      "Std            0.1639         0.1055         0.0809\n",
      "Min           -0.0783         0.1255        -0.2308\n",
      "25%            0.4171         0.3980         0.0648\n",
      "50%            0.5440         0.4759         0.0969\n",
      "75%            0.6514         0.5476         0.1625\n",
      "Max            0.9287         0.7643         0.6667\n",
      "Skipped 493,639 potential negatives (37.32%) due to the absolute_margin of 0.05.\n",
      "Skipped 5 potential negatives (0.00%) due to the max_score of 0.8.\n",
      "Could not find enough negatives for 18442 samples (28.70%). Consider adjusting the range_max, range_min, absolute_margin and max_score parameters if you'd like to find more valid negatives.\n",
      "Setting range_max to 7 based on the provided parameters.\n",
      "Found 1399 unique queries out of 1400 total queries.\n",
      "Found an average of 1.001 positives per query.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  3.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 61.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric       Positive       Negative     Difference\n",
      "Count           1,400          7,000               \n",
      "Mean           0.5248         0.5402        -0.0154\n",
      "Median         0.5435         0.5493        -0.0000\n",
      "Std            0.1603         0.1151         0.1361\n",
      "Min            0.0295         0.1618        -0.6029\n",
      "25%            0.4099         0.4616        -0.0925\n",
      "50%            0.5438         0.5493        -0.0000\n",
      "75%            0.6468         0.6234         0.0614\n",
      "Max            0.8905         0.8905         0.5734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (624 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='58626' max='58626' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [58626/58626 2:53:15, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Dev Set Map</th>\n",
       "      <th>Dev Set Mrr@10</th>\n",
       "      <th>Dev Set Ndcg@10</th>\n",
       "      <th>Dev Set Base Map</th>\n",
       "      <th>Dev Set Base Mrr@10</th>\n",
       "      <th>Dev Set Base Ndcg@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.649500</td>\n",
       "      <td>0.072873</td>\n",
       "      <td>0.775345</td>\n",
       "      <td>0.775345</td>\n",
       "      <td>0.829819</td>\n",
       "      <td>0.465655</td>\n",
       "      <td>0.465655</td>\n",
       "      <td>0.491332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.682700</td>\n",
       "      <td>0.687636</td>\n",
       "      <td>0.644940</td>\n",
       "      <td>0.644940</td>\n",
       "      <td>0.731560</td>\n",
       "      <td>0.465655</td>\n",
       "      <td>0.465655</td>\n",
       "      <td>0.491332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.661800</td>\n",
       "      <td>0.398979</td>\n",
       "      <td>0.413202</td>\n",
       "      <td>0.413321</td>\n",
       "      <td>0.554605</td>\n",
       "      <td>0.465655</td>\n",
       "      <td>0.465655</td>\n",
       "      <td>0.491332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.621900</td>\n",
       "      <td>0.069230</td>\n",
       "      <td>0.749655</td>\n",
       "      <td>0.749655</td>\n",
       "      <td>0.810180</td>\n",
       "      <td>0.465655</td>\n",
       "      <td>0.465655</td>\n",
       "      <td>0.491332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.601500</td>\n",
       "      <td>0.054134</td>\n",
       "      <td>0.762869</td>\n",
       "      <td>0.762869</td>\n",
       "      <td>0.820240</td>\n",
       "      <td>0.465655</td>\n",
       "      <td>0.465655</td>\n",
       "      <td>0.491332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    }
   ],
   "source": [
    "\n",
    "train_batch_size = 2\n",
    "max_length = 512\n",
    "model_to_finetune = \"cross-encoder/ms-marco-electra-base\"\n",
    "\n",
    "exp_runner = ExperimentRunner(\n",
    "    model_to_finetune,\n",
    "    train_batch_size,\n",
    "    num_epochs,\n",
    "    max_length,\n",
    "    experiment_name=experiment_name\n",
    ")\n",
    "exp_runner.run_experiment(dev_dataset, hard_train_dataset, hard_eval_dataset, num_hard_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5120df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (798 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='111332' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [     5/111332 01:39 < 1028:56:32, 0.03 it/s, Epoch 0.00/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='231' max='1400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 231/1400 00:04 < 00:25, 46.42 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      3\u001b[39m model_to_finetune = \u001b[33m\"\u001b[39m\u001b[33mAlibaba-NLP/gte-reranker-modernbert-base\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m exp_runner = ExperimentRunner(\n\u001b[32m      6\u001b[39m     model_to_finetune,\n\u001b[32m      7\u001b[39m     train_batch_size,\n\u001b[32m      8\u001b[39m     num_epochs,\n\u001b[32m      9\u001b[39m     max_length,\n\u001b[32m     10\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mexp_runner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdev_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhard_train_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhard_eval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_hard_negatives\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 105\u001b[39m, in \u001b[36mExperimentRunner.run_experiment\u001b[39m\u001b[34m(self, dev_dataset, hard_train_dataset, hard_eval_dataset, num_hard_negatives, trust_remote_code, eval_steps)\u001b[39m\n\u001b[32m     94\u001b[39m trainer = CrossEncoderTrainer(\n\u001b[32m     95\u001b[39m     model=model,\n\u001b[32m     96\u001b[39m     args=args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    101\u001b[39m \n\u001b[32m    102\u001b[39m )\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m model.save(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodels/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.model_to_finetune.split(\u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[33m'\u001b[39m)[-\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-finetuned\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/trainer.py:2627\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2625\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.epoch = epoch + (step + \u001b[32m1\u001b[39m + steps_skipped) / steps_in_epoch\n\u001b[32m   2626\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2627\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2634\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2635\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2636\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2637\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2638\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_substep_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/trainer.py:3096\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[39m\n\u001b[32m   3094\u001b[39m metrics = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3095\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_evaluate:\n\u001b[32m-> \u001b[39m\u001b[32m3096\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3097\u001b[39m     is_new_best_metric = \u001b[38;5;28mself\u001b[39m._determine_best_metric(metrics=metrics, trial=trial)\n\u001b[32m   3099\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_strategy == SaveStrategy.BEST:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/trainer.py:3045\u001b[39m, in \u001b[36mTrainer._evaluate\u001b[39m\u001b[34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[39m\n\u001b[32m   3044\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m3045\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3046\u001b[39m     \u001b[38;5;28mself\u001b[39m._report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m.state.global_step, metrics)\n\u001b[32m   3048\u001b[39m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sentence_transformers/trainer.py:462\u001b[39m, in \u001b[36mSentenceTransformerTrainer.evaluate\u001b[39m\u001b[34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m    460\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    461\u001b[39m     eval_dataset = \u001b[38;5;28mself\u001b[39m.eval_dataset\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/trainer.py:4154\u001b[39m, in \u001b[36mTrainer.evaluate\u001b[39m\u001b[34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4151\u001b[39m start_time = time.time()\n\u001b[32m   4153\u001b[39m eval_loop = \u001b[38;5;28mself\u001b[39m.prediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.use_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.evaluation_loop\n\u001b[32m-> \u001b[39m\u001b[32m4154\u001b[39m output = \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4155\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4156\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEvaluation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4157\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[32m   4158\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[32m   4159\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4160\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4161\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4162\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4164\u001b[39m total_batch_size = \u001b[38;5;28mself\u001b[39m.args.eval_batch_size * \u001b[38;5;28mself\u001b[39m.args.world_size\n\u001b[32m   4165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_jit_compilation_time\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output.metrics:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sentence_transformers/trainer.py:472\u001b[39m, in \u001b[36mSentenceTransformerTrainer.evaluation_loop\u001b[39m\u001b[34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluation_loop\u001b[39m(\n\u001b[32m    465\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    466\u001b[39m     dataloader: DataLoader,\n\u001b[32m   (...)\u001b[39m\u001b[32m    470\u001b[39m     metric_key_prefix: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33meval\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    471\u001b[39m ) -> EvalLoopOutput:\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m     output = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluation_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m     \u001b[38;5;66;03m# If the evaluator is not defined, we can just return the output\u001b[39;00m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.evaluator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/trainer.py:4348\u001b[39m, in \u001b[36mTrainer.evaluation_loop\u001b[39m\u001b[34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4345\u001b[39m         batch_size = observed_batch_size\n\u001b[32m   4347\u001b[39m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4348\u001b[39m losses, logits, labels = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4349\u001b[39m main_input_name = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, \u001b[33m\"\u001b[39m\u001b[33mmain_input_name\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4350\u001b[39m inputs_decode = (\n\u001b[32m   4351\u001b[39m     \u001b[38;5;28mself\u001b[39m._prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33minputs\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args.include_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4352\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/trainer.py:4564\u001b[39m, in \u001b[36mTrainer.prediction_step\u001b[39m\u001b[34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[39m\n\u001b[32m   4562\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_labels \u001b[38;5;129;01mor\u001b[39;00m loss_without_labels:\n\u001b[32m   4563\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m4564\u001b[39m         loss, outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   4565\u001b[39m     loss = loss.detach().mean()\n\u001b[32m   4567\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sentence_transformers/trainer.py:406\u001b[39m, in \u001b[36mSentenceTransformerTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    401\u001b[39m     model == \u001b[38;5;28mself\u001b[39m.model_wrapped\n\u001b[32m    402\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(loss_fn, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# Only if the loss stores the model\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m loss_fn.model != model  \u001b[38;5;66;03m# Only if the wrapped model is not already stored\u001b[39;00m\n\u001b[32m    404\u001b[39m ):\n\u001b[32m    405\u001b[39m     loss_fn = \u001b[38;5;28mself\u001b[39m.override_model_in_loss(loss_fn, model)\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m loss = \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_outputs:\n\u001b[32m    408\u001b[39m     \u001b[38;5;66;03m# During prediction/evaluation, `compute_loss` will be called with `return_outputs=True`.\u001b[39;00m\n\u001b[32m    409\u001b[39m     \u001b[38;5;66;03m# However, Sentence Transformer losses do not return outputs, so we return an empty dictionary.\u001b[39;00m\n\u001b[32m    410\u001b[39m     \u001b[38;5;66;03m# This does not result in any problems, as the SentenceTransformerTrainingArguments sets\u001b[39;00m\n\u001b[32m    411\u001b[39m     \u001b[38;5;66;03m# `prediction_loss_only=True` which means that the output is not used.\u001b[39;00m\n\u001b[32m    412\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss, {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sentence_transformers/cross_encoder/losses/MultipleNegativesRankingLoss.py:174\u001b[39m, in \u001b[36mMultipleNegativesRankingLoss.forward\u001b[39m\u001b[34m(self, inputs, labels)\u001b[39m\n\u001b[32m    171\u001b[39m positives = inputs[\u001b[32m1\u001b[39m]\n\u001b[32m    172\u001b[39m batch_size = \u001b[38;5;28mlen\u001b[39m(anchors)\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m scores = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_model_with_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43manchors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositives\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[32m    176\u001b[39m \u001b[38;5;66;03m# In-batch negatives:\u001b[39;00m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m negatives \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_in_batch_negatives(anchors, inputs[\u001b[32m1\u001b[39m:]):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sentence_transformers/cross_encoder/losses/MultipleNegativesRankingLoss.py:116\u001b[39m, in \u001b[36mMultipleNegativesRankingLoss.call_model_with_columns\u001b[39m\u001b[34m(self, anchors, candidates)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_model_with_columns\u001b[39m(\u001b[38;5;28mself\u001b[39m, anchors: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m], candidates: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]) -> Tensor:\n\u001b[32m    115\u001b[39m     pairs = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(anchors, candidates))\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_model_with_pairs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sentence_transformers/cross_encoder/losses/MultipleNegativesRankingLoss.py:126\u001b[39m, in \u001b[36mMultipleNegativesRankingLoss.call_model_with_pairs\u001b[39m\u001b[34m(self, pairs)\u001b[39m\n\u001b[32m    119\u001b[39m tokens = \u001b[38;5;28mself\u001b[39m.model.tokenizer(\n\u001b[32m    120\u001b[39m     pairs,\n\u001b[32m    121\u001b[39m     padding=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    122\u001b[39m     truncation=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    123\u001b[39m     return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    124\u001b[39m )\n\u001b[32m    125\u001b[39m tokens.to(\u001b[38;5;28mself\u001b[39m.model.device)\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m logits.squeeze(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/accelerate/utils/operations.py:814\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    813\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m814\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/accelerate/utils/operations.py:802\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m802\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/amp/autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:529\u001b[39m, in \u001b[36mCrossEncoder.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    528\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m529\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/models/modernbert/modeling_modernbert.py:1225\u001b[39m, in \u001b[36mModernBertForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, sliding_window_mask, position_ids, inputs_embeds, labels, indices, cu_seqlens, max_seqlen, batch_size, seq_len, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m   1222\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m   1223\u001b[39m \u001b[38;5;28mself\u001b[39m._maybe_set_compile()\n\u001b[32m-> \u001b[39m\u001b[32m1225\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m    \u001b[49m\u001b[43msliding_window_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43msliding_window_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seqlen\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_seqlen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1240\u001b[39m last_hidden_state = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.classifier_pooling == \u001b[33m\"\u001b[39m\u001b[33mcls\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/models/modernbert/modeling_modernbert.py:962\u001b[39m, in \u001b[36mModernBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, sliding_window_mask, position_ids, inputs_embeds, indices, cu_seqlens, max_seqlen, batch_size, seq_len, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    951\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    952\u001b[39m         encoder_layer.\u001b[34m__call__\u001b[39m,\n\u001b[32m    953\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    959\u001b[39m         output_attentions,\n\u001b[32m    960\u001b[39m     )\n\u001b[32m    961\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m962\u001b[39m     layer_outputs = \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43msliding_window_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43msliding_window_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_seqlen\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_seqlen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    971\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    972\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(layer_outputs) > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/models/modernbert/modeling_modernbert.py:545\u001b[39m, in \u001b[36mModernBertEncoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, sliding_window_mask, position_ids, cu_seqlens, max_seqlen, output_attentions)\u001b[39m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    537\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    543\u001b[39m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    544\u001b[39m ) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m545\u001b[39m     attn_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m        \u001b[49m\u001b[43msliding_window_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43msliding_window_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_seqlen\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_seqlen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    554\u001b[39m     hidden_states = hidden_states + attn_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    555\u001b[39m     mlp_output = (\n\u001b[32m    556\u001b[39m         \u001b[38;5;28mself\u001b[39m.compiled_mlp(hidden_states)\n\u001b[32m    557\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.reference_compile\n\u001b[32m    558\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mlp(\u001b[38;5;28mself\u001b[39m.mlp_norm(hidden_states))\n\u001b[32m    559\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/models/modernbert/modeling_modernbert.py:503\u001b[39m, in \u001b[36mModernBertAttention.forward\u001b[39m\u001b[34m(self, hidden_states, output_attentions, **kwargs)\u001b[39m\n\u001b[32m    500\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    501\u001b[39m     qkv = qkv.view(bs, -\u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m, \u001b[38;5;28mself\u001b[39m.num_heads, \u001b[38;5;28mself\u001b[39m.head_dim)\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m attn_outputs = \u001b[43mMODERNBERT_ATTENTION_FUNCTION\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_attn_implementation\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m    \u001b[49m\u001b[43mqkv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mqkv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrotary_emb\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrotary_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_attention\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlocal_attention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mall_head_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    513\u001b[39m hidden_states = attn_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    514\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.out_drop(\u001b[38;5;28mself\u001b[39m.Wo(hidden_states))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/models/modernbert/modeling_modernbert.py:409\u001b[39m, in \u001b[36msdpa_attention_forward\u001b[39m\u001b[34m(module, qkv, attention_mask, sliding_window_mask, position_ids, local_attention, bs, dim, **_kwargs)\u001b[39m\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msdpa_attention_forward\u001b[39m(\n\u001b[32m    398\u001b[39m     module: \u001b[33m\"\u001b[39m\u001b[33mModernBertAttention\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    399\u001b[39m     qkv: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    407\u001b[39m ) -> Tuple[torch.Tensor]:\n\u001b[32m    408\u001b[39m     \u001b[38;5;66;03m# qkv: [batch_size, seqlen, 3, nheads, headdim]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     cos, sin = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrotary_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqkv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m     query, key, value = qkv.transpose(\u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m).unbind(dim=\u001b[32m2\u001b[39m)\n\u001b[32m    411\u001b[39m     \u001b[38;5;66;03m# query, key, value: [batch_size, heads, seq_len, head_dim]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/modeling_rope_utils.py:87\u001b[39m, in \u001b[36mdynamic_rope_update.<locals>.wrapper\u001b[39m\u001b[34m(self, x, position_ids)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.rope_type == \u001b[33m\"\u001b[39m\u001b[33mlongrope\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     86\u001b[39m     longrope_frequency_update(\u001b[38;5;28mself\u001b[39m, position_ids, device=x.device)\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrope_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/models/modernbert/modeling_modernbert.py:278\u001b[39m, in \u001b[36mModernBertRotaryEmbedding.forward\u001b[39m\u001b[34m(self, x, position_ids)\u001b[39m\n\u001b[32m    276\u001b[39m     emb = torch.cat((freqs, freqs), dim=-\u001b[32m1\u001b[39m)\n\u001b[32m    277\u001b[39m     cos = emb.cos() * \u001b[38;5;28mself\u001b[39m.attention_scaling\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m     sin = \u001b[43memb\u001b[49m\u001b[43m.\u001b[49m\u001b[43msin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m * \u001b[38;5;28mself\u001b[39m.attention_scaling\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_batch_size = 1\n",
    "max_length = 512\n",
    "model_to_finetune = \"Alibaba-NLP/gte-reranker-modernbert-base\"\n",
    "\n",
    "exp_runner = ExperimentRunner(\n",
    "    model_to_finetune,\n",
    "    train_batch_size,\n",
    "    num_epochs,\n",
    "    max_length,\n",
    "    experiment_name=experiment_name\n",
    ")\n",
    "exp_runner.run_experiment(dev_dataset, hard_train_dataset, hard_eval_dataset, num_hard_negatives, eval_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15f90a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m num_hard_negatives = \u001b[32m3\u001b[39m\n\u001b[32m      2\u001b[39m num_hard_negatives_eval = \u001b[32m3\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m hard_train_dataset, hard_eval_dataset = \u001b[43mget_hard_negatives\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_model_hard_negatives\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_hard_negatives\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_hard_negatives\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_hard_negatives_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_hard_negatives_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdev_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_full\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf_full\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m train_batch_size = \u001b[32m1\u001b[39m\n\u001b[32m      7\u001b[39m max_length = \u001b[32m512\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mget_hard_negatives\u001b[39m\u001b[34m(embedding_model_hard_negatives, num_hard_negatives, num_hard_negatives_eval, train_dataset, dev_dataset, df_full, batch_size)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_hard_negatives\u001b[39m(embedding_model_hard_negatives, num_hard_negatives, num_hard_negatives_eval, train_dataset, dev_dataset, df_full, batch_size=\u001b[32m4096\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     embedding_model = \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_model_hard_negatives\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     hard_train_dataset = mine_hard_negatives(\n\u001b[32m      6\u001b[39m         train_dataset,\n\u001b[32m      7\u001b[39m         embedding_model,\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m         output_format=\u001b[33m\"\u001b[39m\u001b[33mlabeled-pair\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# The output format is (query, passage, label), as required by BinaryCrossEntropyLoss\u001b[39;00m\n\u001b[32m     16\u001b[39m     )\n\u001b[32m     18\u001b[39m     hard_eval_dataset = mine_hard_negatives(\n\u001b[32m     19\u001b[39m         dev_dataset,\n\u001b[32m     20\u001b[39m         embedding_model,\n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m         sampling_strategy=\u001b[33m\"\u001b[39m\u001b[33mtop\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# Randomly sample negatives from the range\u001b[39;00m\n\u001b[32m     31\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:309\u001b[39m, in \u001b[36mSentenceTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[39m\n\u001b[32m    300\u001b[39m         model_name_or_path = __MODEL_HUB_ORGANIZATION__ + \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m + model_name_or_path\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentence_transformer_model(\n\u001b[32m    303\u001b[39m     model_name_or_path,\n\u001b[32m    304\u001b[39m     token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    307\u001b[39m     local_files_only=local_files_only,\n\u001b[32m    308\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m     modules, \u001b[38;5;28mself\u001b[39m.module_kwargs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    321\u001b[39m     modules = \u001b[38;5;28mself\u001b[39m._load_auto_model(\n\u001b[32m    322\u001b[39m         model_name_or_path,\n\u001b[32m    323\u001b[39m         token=token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    330\u001b[39m         config_kwargs=config_kwargs,\n\u001b[32m    331\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:1808\u001b[39m, in \u001b[36mSentenceTransformer._load_sbert_model\u001b[39m\u001b[34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[39m\n\u001b[32m   1805\u001b[39m \u001b[38;5;66;03m# Try to initialize the module with a lot of kwargs, but only if the module supports them\u001b[39;00m\n\u001b[32m   1806\u001b[39m \u001b[38;5;66;03m# Otherwise we fall back to the load method\u001b[39;00m\n\u001b[32m   1807\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1808\u001b[39m     module = \u001b[43mmodule_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1809\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   1810\u001b[39m     module = module_class.load(model_name_or_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:80\u001b[39m, in \u001b[36mTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     78\u001b[39m     config_args = {}\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m config, is_peft_model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;28mself\u001b[39m._load_model(model_name_or_path, config, cache_dir, backend, is_peft_model, **model_args)\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:145\u001b[39m, in \u001b[36mTransformer._load_config\u001b[39m\u001b[34m(self, model_name_or_path, cache_dir, backend, config_args)\u001b[39m\n\u001b[32m    141\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftConfig\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m PeftConfig.from_pretrained(model_name_or_path, **config_args, cache_dir=cache_dir), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py:1114\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1111\u001b[39m trust_remote_code = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mtrust_remote_code\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   1112\u001b[39m code_revision = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mcode_revision\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1114\u001b[39m config_dict, unused_kwargs = \u001b[43mPretrainedConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1115\u001b[39m has_remote_code = \u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAutoConfig\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1116\u001b[39m has_local_code = \u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/configuration_utils.py:590\u001b[39m, in \u001b[36mPretrainedConfig.get_config_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    588\u001b[39m original_kwargs = copy.deepcopy(kwargs)\n\u001b[32m    589\u001b[39m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m590\u001b[39m config_dict, kwargs = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    592\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {}, kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/configuration_utils.py:649\u001b[39m, in \u001b[36mPretrainedConfig._get_config_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    645\u001b[39m configuration_file = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33m_configuration_file\u001b[39m\u001b[33m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[32m    647\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    648\u001b[39m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m649\u001b[39m     resolved_config_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    661\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    662\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    663\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    664\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/utils/hub.py:266\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    209\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    210\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    211\u001b[39m     **kwargs,\n\u001b[32m    212\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    213\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    214\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    215\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    264\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    268\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/utils/hub.py:424\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    422\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    423\u001b[39m         \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m424\u001b[39m         \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    438\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    439\u001b[39m         snapshot_download(\n\u001b[32m    440\u001b[39m             path_or_repo_id,\n\u001b[32m    441\u001b[39m             allow_patterns=full_filenames,\n\u001b[32m   (...)\u001b[39m\u001b[32m    450\u001b[39m             local_files_only=local_files_only,\n\u001b[32m    451\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/huggingface_hub/file_download.py:1008\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m    988\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[32m    989\u001b[39m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[32m    990\u001b[39m         local_dir=local_dir,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1005\u001b[39m         local_files_only=local_files_only,\n\u001b[32m   1006\u001b[39m     )\n\u001b[32m   1007\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1008\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/huggingface_hub/file_download.py:1071\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1067\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n\u001b[32m   1069\u001b[39m \u001b[38;5;66;03m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[39;00m\n\u001b[32m   1070\u001b[39m \u001b[38;5;66;03m# If we can't, a HEAD request error is returned.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1071\u001b[39m (url_to_download, etag, commit_hash, expected_size, xet_file_data, head_call_error) = \u001b[43m_get_metadata_or_catch_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1078\u001b[39m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1083\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1084\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[38;5;66;03m# etag can be None for several reasons:\u001b[39;00m\n\u001b[32m   1087\u001b[39m \u001b[38;5;66;03m# 1. we passed local_files_only.\u001b[39;00m\n\u001b[32m   1088\u001b[39m \u001b[38;5;66;03m# 2. we don't have a connection\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1094\u001b[39m \u001b[38;5;66;03m# If the specified revision is a commit hash, look inside \"snapshots\".\u001b[39;00m\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# If the specified revision is a branch or tag, look inside \"refs\".\u001b[39;00m\n\u001b[32m   1096\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m head_call_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1097\u001b[39m     \u001b[38;5;66;03m# Couldn't make a HEAD call => let's try to find a local file\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/huggingface_hub/file_download.py:1531\u001b[39m, in \u001b[36m_get_metadata_or_catch_error\u001b[39m\u001b[34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[39m\n\u001b[32m   1529\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1530\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1531\u001b[39m         metadata = \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1532\u001b[39m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\n\u001b[32m   1533\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1534\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[32m   1535\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m storage_folder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m relative_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1536\u001b[39m             \u001b[38;5;66;03m# Cache the non-existence of the file\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/huggingface_hub/file_download.py:1448\u001b[39m, in \u001b[36mget_hf_file_metadata\u001b[39m\u001b[34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[39m\n\u001b[32m   1445\u001b[39m hf_headers[\u001b[33m\"\u001b[39m\u001b[33mAccept-Encoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33midentity\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[32m   1447\u001b[39m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1448\u001b[39m r = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1449\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHEAD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1450\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1451\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1452\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1453\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1454\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1455\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1456\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1457\u001b[39m hf_raise_for_status(r)\n\u001b[32m   1459\u001b[39m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/huggingface_hub/file_download.py:286\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;66;03m# Recursively follow relative redirects\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     response = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    293\u001b[39m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[32m    294\u001b[39m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[32m    295\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m300\u001b[39m <= response.status_code <= \u001b[32m399\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/huggingface_hub/file_download.py:309\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m    308\u001b[39m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m response = \u001b[43mhttp_backoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_on_exceptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_on_status_codes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m429\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m hf_raise_for_status(response)\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:310\u001b[39m, in \u001b[36mhttp_backoff\u001b[39m\u001b[34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[39m\n\u001b[32m    307\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m].seek(io_obj_initial_pos)\n\u001b[32m    309\u001b[39m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m response = \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m retry_on_status_codes:\n\u001b[32m    312\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:96\u001b[39m, in \u001b[36mUniqueRequestIdAdapter.send\u001b[39m\u001b[34m(self, request, *args, **kwargs)\u001b[39m\n\u001b[32m     94\u001b[39m     logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSend: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_curlify(request)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.RequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     98\u001b[39m     request_id = request.headers.get(X_AMZN_TRACE_ID)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/requests/adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    664\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/urllib3/connection.py:516\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    513\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    515\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    519\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/http/client.py:1430\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1428\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1429\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1430\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1431\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1432\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/http/client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/http/client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    722\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/ssl.py:1251\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1247\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1248\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1249\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1250\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/ssl.py:1103\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1102\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "num_hard_negatives = 3\n",
    "num_hard_negatives_eval = 3\n",
    "\n",
    "hard_train_dataset, hard_eval_dataset = get_hard_negatives(embedding_model_hard_negatives, num_hard_negatives=num_hard_negatives, num_hard_negatives_eval=num_hard_negatives_eval, train_dataset=train_dataset, dev_dataset=dev_dataset, df_full=df_full)\n",
    "\n",
    "train_batch_size = 1\n",
    "max_length = 512\n",
    "model_to_finetune = \"Alibaba-NLP/gte-multilingual-reranker-base\"\n",
    "\n",
    "exp_runner = ExperimentRunner(\n",
    "    model_to_finetune,\n",
    "    train_batch_size,\n",
    "    num_epochs,\n",
    "    max_length,\n",
    "    experiment_name=experiment_name\n",
    ")\n",
    "exp_runner.run_experiment(dev_dataset, hard_train_dataset, hard_eval_dataset, num_hard_negatives, eval_steps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cf480d",
   "metadata": {},
   "source": [
    "Note that the metrics in the following cells are overly optimistic, since we evaluate on the whole data, which includes the training data. We do this on the whole data so that we have enough data for the ensemble learning part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2afedbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reranking: 100%|██████████| 14253/14253 [1:12:52<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1\n",
      "Number of queries in top 1: 9609\n",
      "Number of queries not in top 1: 4644\n",
      "k = 5\n",
      "Number of queries in top 5: 11089\n",
      "Number of queries not in top 5: 3164\n",
      "k = 10\n",
      "Number of queries in top 10: 11349\n",
      "Number of queries not in top 10: 2904\n",
      "Evaluation results: {1: 0.6741738581351294, 5: 0.7166327556771674, 10: 0.7191217053719685}\n"
     ]
    }
   ],
   "source": [
    "df_query_single = experiment_single_bm25(df_collection, df_query_combined, stemmer=stemmer, k=30)\n",
    "model_name = \"models/gte-reranker-modernbert-base-finetuned\"\n",
    "tokenizer, model = initialize_reranker(model_name)\n",
    "df_query = rerank_with_alibaba(df_query_single, df_collection, tokenizer, model, k=10, batch_size=100)\n",
    "df_query.to_parquet('data/reranked_results_alibaba_finetuned.parquet')\n",
    "results = evaluate_reranked_results(df_query)\n",
    "print(f\"Evaluation results: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1871660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reranking: 100%|██████████| 14253/14253 [1:02:52<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1\n",
      "Number of queries in top 1: 9021\n",
      "Number of queries not in top 1: 5232\n",
      "k = 5\n",
      "Number of queries in top 5: 10842\n",
      "Number of queries not in top 5: 3411\n",
      "k = 10\n",
      "Number of queries in top 10: 11232\n",
      "Number of queries not in top 10: 3021\n",
      "Evaluation results: {1: 0.6329193853925489, 5: 0.6854755723941158, 10: 0.6892080085618287}\n"
     ]
    }
   ],
   "source": [
    "df_query_dev_single = experiment_single_bm25(df_collection, df_query_combined, stemmer=stemmer, k=30)\n",
    "model_name = \"models/gte-multilingual-reranker-base-finetuned\"\n",
    "tokenizer, model = initialize_reranker(model_name)\n",
    "df_query = rerank_with_alibaba(df_query_dev_single, df_collection, tokenizer, model, k=10, batch_size=100)\n",
    "df_query.to_parquet('data/reranked_results_alibaba_multilingual_finetuned.parquet')\n",
    "results = evaluate_reranked_results(df_query)\n",
    "print(f\"Evaluation results: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f615c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reranking: 100%|██████████| 14253/14253 [25:38<00:00,  9.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1\n",
      "Number of queries in top 1: 9429\n",
      "Number of queries not in top 1: 4824\n",
      "k = 5\n",
      "Number of queries in top 5: 11000\n",
      "Number of queries not in top 5: 3253\n",
      "k = 10\n",
      "Number of queries in top 10: 11312\n",
      "Number of queries not in top 10: 2941\n",
      "Evaluation results: {1: 0.6615449379078089, 5: 0.7066407072195328, 10: 0.7096312778484952}\n"
     ]
    }
   ],
   "source": [
    "df_query_dev_single = experiment_single_bm25(df_collection, df_query_combined, stemmer=stemmer, k=30)\n",
    "model_name = f\"models/ms-marco-MiniLM-L12-v2-finetuned{experiment_name}\"\n",
    "tokenizer, model = initialize_reranker(model_name)\n",
    "df_query = rerank_with_alibaba(df_query_dev_single, df_collection, tokenizer, model, k=10, batch_size=100)\n",
    "df_query.to_parquet(f\"data/reranked_results_miniLM12_finetuned{experiment_name}.parquet\")\n",
    "results = evaluate_reranked_results(df_query)\n",
    "print(f\"Evaluation results: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1726416c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reranking: 100%|██████████| 14253/14253 [18:15<00:00, 13.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1\n",
      "Number of queries in top 1: 8935\n",
      "Number of queries not in top 1: 5318\n",
      "k = 5\n",
      "Number of queries in top 5: 10804\n",
      "Number of queries not in top 5: 3449\n",
      "k = 10\n",
      "Number of queries in top 10: 11201\n",
      "Number of queries not in top 10: 3052\n",
      "Evaluation results: {1: 0.6268855679506069, 5: 0.6795516733319301, 10: 0.6833366186344506}\n"
     ]
    }
   ],
   "source": [
    "df_query_dev_single = experiment_single_bm25(df_collection, df_query_combined, stemmer=stemmer, k=30)\n",
    "model_name = f\"models/ms-marco-MiniLM-L6-v2-finetuned{experiment_name}\"\n",
    "tokenizer, model = initialize_reranker(model_name)\n",
    "df_query = rerank_with_alibaba(df_query_dev_single, df_collection, tokenizer, model, k=10, batch_size=100)\n",
    "df_query.to_parquet(f\"data/reranked_results_miniLM6_finetuned.parquet{experiment_name}\")\n",
    "results = evaluate_reranked_results(df_query)\n",
    "print(f\"Evaluation results: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bc22bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reranking: 100%|██████████| 14253/14253 [11:47<00:00, 20.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1\n",
      "Number of queries in top 1: 8159\n",
      "Number of queries not in top 1: 6094\n",
      "k = 5\n",
      "Number of queries in top 5: 10348\n",
      "Number of queries not in top 5: 3905\n",
      "k = 10\n",
      "Number of queries in top 10: 10970\n",
      "Number of queries not in top 10: 3283\n",
      "Evaluation results: {1: 0.5724408896372694, 5: 0.6330105942608573, 10: 0.6389655923175181}\n"
     ]
    }
   ],
   "source": [
    "df_query_dev_single = experiment_single_bm25(df_collection, df_query_combined, stemmer=stemmer, k=30)\n",
    "model_name = f\"models/ms-marco-TinyBERT-L2-v2-finetuned{experiment_name}\"\n",
    "tokenizer, model = initialize_reranker(model_name)\n",
    "df_query = rerank_with_alibaba(df_query_dev_single, df_collection, tokenizer, model, k=10, batch_size=100)\n",
    "df_query.to_parquet(f\"data/reranked_results_tinyBERT_finetuned{experiment_name}.parquet\")\n",
    "results = evaluate_reranked_results(df_query)\n",
    "print(f\"Evaluation results: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b096aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reranking: 100%|██████████| 14253/14253 [59:01<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1\n",
      "Number of queries in top 1: 7957\n",
      "Number of queries not in top 1: 6296\n",
      "k = 5\n",
      "Number of queries in top 5: 10281\n",
      "Number of queries not in top 5: 3972\n",
      "k = 10\n",
      "Number of queries in top 10: 10933\n",
      "Number of queries not in top 10: 3320\n",
      "Evaluation results: {1: 0.5582684347154985, 5: 0.6218129516592997, 10: 0.6280638495488}\n"
     ]
    }
   ],
   "source": [
    "df_query_dev_single = experiment_single_bm25(df_collection, df_query_combined, stemmer=stemmer, k=30)\n",
    "model_name = f\"models/ms-marco-electra-base-finetuned{experiment_name}\"\n",
    "tokenizer, model = initialize_reranker(model_name)\n",
    "df_query = rerank_with_alibaba(df_query_dev_single, df_collection, tokenizer, model, k=10, batch_size=50, max_length=None )\n",
    "df_query.to_parquet(f\"data/reranked_results_electra_finetuned.parquet{experiment_name}\")\n",
    "results = evaluate_reranked_results(df_query)\n",
    "print(f\"Evaluation results: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd9ba0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
